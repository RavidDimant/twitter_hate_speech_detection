{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Support Vector Machine (SVM) Modeling\n",
    "\n",
    "In this notebook, we iterate through an SVM baseline, trying different class imbalance remedy methods. We also grid search to try and optomize the baseline's hyperparameters.\n",
    "\n",
    "The idea behind SVMs is that you perform classification by finding the seperation line or (in higher dimensions) 'hyperplane' that maximizes the distance between two classes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "from sklearn.feature_extraction import text \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn import metrics, model_selection, svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, plot_confusion_matrix, roc_curve, auc, classification_report\n",
    "import pickle"
   ]
  },
  {
   "source": [
    "## Importing X and y from `nlp_preprocessing.ipynb`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lem = pickle.load(open('../pickle/X_lem.pkl', 'rb'))\n",
    "y_lem = pd.read_pickle('../pickle/y_lem.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up stop words\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "source": [
    "## Train-Test Split & Vectorize"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_lem, y_lem, test_size=0.20, random_state=15)\n",
    "\n",
    "# using tf_idf vectorizor with bigrams\n",
    "tfidf = TfidfVectorizer(stop_words= stop_words, ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse matrix format with 265K stored elements\n",
    "tfidf_data_train = tfidf.fit_transform(X_train)\n",
    "tfidf_data_test = tfidf.transform(X_test)"
   ]
  },
  {
   "source": [
    "## SVM Baseline\n",
    "\n",
    "SVM Hyperparameters:\n",
    "- `C` is the regularization parameter, `1.0` is the default.\n",
    "- `kernel` specifies the kernal type to be used in the algorithm, default is `rbf`. These are different ways of drawing non-linear boundaries around classes.\n",
    "- `degree` is the degree of the polynomial kernal functions (`poly`), ignored by all other kernals.\n",
    "- `gamma` is the kernal coefficient for `rbf`, `poly` and `sigmoid`, default is `scale`.\n",
    "- 'class_weight' default 1. If balanced, it uses the values of y to automatically adjust weights inversely proportional to class frequencies in the output data as `n_samples / (n_classes * np.bincount(y))`.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_baseline = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto', class_weight='balanced', random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 51.5 s, sys: 1.05 s, total: 52.5 s\nWall time: 53.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# this cell takes about 53 seconds to run\n",
    "# fit the training dataset on the classifier\n",
    "SVM_baseline.fit(tfidf_data_train, y_train)\n",
    "# predict the labels on validation dataset\n",
    "SVM_test_preds = SVM_baseline.predict(tfidf_data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_precision = precision_score(y_test, SVM_test_preds)\n",
    "baseline_recall = recall_score(y_test, SVM_test_preds)\n",
    "baseline_f1_score = f1_score(y_test, SVM_test_preds)\n",
    "baseline_weighted_f1_score = f1_score(y_test, SVM_test_preds, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Metrics for SVM Baseline with Lemmatization & TF-IDF Vectorization\nPrecision: 0.3609\nRecall: 0.4373\nF1 Score: 0.3955\nWeighted F1 Score: 0.9281\n"
     ]
    }
   ],
   "source": [
    "# printing evaluation metrics up to 4th decimal place\n",
    "print('Testing Metrics for SVM Baseline with Lemmatization & TF-IDF Vectorization')\n",
    "print('Precision: {:.4}'.format(baseline_precision))\n",
    "print('Recall: {:.4}'.format(baseline_recall))\n",
    "print('F1 Score: {:.4}'.format(baseline_f1_score))\n",
    "print('Weighted F1 Score: {:.4}'.format(baseline_weighted_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dictionary with all metrics\n",
    "metric_dict = {}\n",
    "metric_dict['Baseline SVM'] = {'precision': baseline_precision, 'recall': baseline_recall, 'f1_score': baseline_f1_score, 'weighted_f1': baseline_weighted_f1_score}"
   ]
  },
  {
   "source": [
    "## Baseline with SMOTE\n",
    "Used to over-sample the minority class (hate speech)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=35)\n",
    "smote_X_train, smote_y_train = sm.fit_sample(tfidf_data_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto', random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 3min 57s, sys: 1.96 s, total: 3min 59s\nWall time: 4min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# this cell takes about 4 minutes to run\n",
    "smote_SVM.fit(smote_X_train, smote_y_train)\n",
    "smote_SVM_test_preds = smote_SVM.predict(tfidf_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_precision = precision_score(y_test, smote_SVM_test_preds)\n",
    "smote_recall = recall_score(y_test, smote_SVM_test_preds)\n",
    "smote_f1_score = f1_score(y_test, smote_SVM_test_preds)\n",
    "smote_weighted_f1_score = f1_score(y_test, smote_SVM_test_preds, average='weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Metrics for Oversampled SVM Baseline with Lemmatization\nPrecision: 0.3393\nRecall: 0.2724\nF1 Score: 0.3022\nWeighted F1 Score: 0.9255\n"
     ]
    }
   ],
   "source": [
    "# printing evaluation metrics up to 4th decimal place\n",
    "print('Testing Metrics for Oversampled SVM Baseline with Lemmatization')\n",
    "print('Precision: {:.4}'.format(smote_precision))\n",
    "print('Recall: {:.4}'.format(smote_recall))\n",
    "print('F1 Score: {:.4}'.format(smote_f1_score))\n",
    "print('Weighted F1 Score: {:.4}'.format(smote_weighted_f1_score))"
   ]
  },
  {
   "source": [
    "Looks like SMOTE actually decreased the F1, which also happened with Logistic Regression."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding these metrics to evaluation metric dict\n",
    "metric_dict['Baseline SVM Oversampled with SMOTE'] = {'precision': smote_precision, 'recall': smote_recall, 'f1_score': smote_f1_score, 'weighted_f1': smote_weighted_f1_score}"
   ]
  },
  {
   "source": [
    "## Baseline with Tomek Links\n",
    "Used to under-sample the majority class (not hate speech)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Resampled dataset shape Counter({0: 18627, 1: 1151})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from imblearn.under_sampling import TomekLinks # doctest: +NORMALIZE_WHITESPACE\n",
    "\n",
    "tl = TomekLinks()\n",
    "tomek_X_train, tomek_y_train = tl.fit_resample(tfidf_data_train, y_train)\n",
    "print('Resampled dataset shape %s' % Counter(tomek_y_train))"
   ]
  },
  {
   "source": [
    "Only removed 48 values from the majority class."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomek_SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto', random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 40.6 s, sys: 564 ms, total: 41.2 s\nWall time: 41.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# this cell takes 42 seconds to run\n",
    "tomek_SVM.fit(tomek_X_train, tomek_y_train)\n",
    "tomek_logreg_test_preds = tomek_SVM.predict(tfidf_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomek_precision = precision_score(y_test, tomek_logreg_test_preds)\n",
    "tomek_recall = recall_score(y_test, tomek_logreg_test_preds)\n",
    "tomek_f1_score = f1_score(y_test, tomek_logreg_test_preds)\n",
    "tomek_weighted_f1_score = f1_score(y_test, tomek_logreg_test_preds, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Metrics for Undersampled SVM Baseline with Lemmatization\nPrecision: 0.6562\nRecall: 0.2258\nF1 Score: 0.336\nF1 Score: 0.938\n"
     ]
    }
   ],
   "source": [
    "# printing evaluation metrics up to 4th decimal place\n",
    "print('Testing Metrics for Undersampled SVM Baseline with Lemmatization')\n",
    "print('Precision: {:.4}'.format(tomek_precision))\n",
    "print('Recall: {:.4}'.format(tomek_recall))\n",
    "print('F1 Score: {:.4}'.format(tomek_f1_score))\n",
    "print('F1 Score: {:.4}'.format(tomek_weighted_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding these metrics to evaluation metric dict\n",
    "metric_dict['Baseline SVM Undersampled with Tomek Links'] = {'precision': tomek_precision, 'recall': tomek_recall, 'f1_score': tomek_f1_score, 'weighted_f1': tomek_weighted_f1_score}"
   ]
  },
  {
   "source": [
    "## Metrics for All Baselines"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                            precision    recall  f1_score  \\\n",
       "Baseline SVM                                 0.360947  0.437276  0.395462   \n",
       "Baseline SVM Oversampled with SMOTE          0.339286  0.272401  0.302187   \n",
       "Baseline SVM Undersampled with Tomek Links   0.656250  0.225806  0.336000   \n",
       "\n",
       "                                            weighted_f1  \n",
       "Baseline SVM                                   0.928112  \n",
       "Baseline SVM Oversampled with SMOTE            0.925527  \n",
       "Baseline SVM Undersampled with Tomek Links     0.937993  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>f1_score</th>\n      <th>weighted_f1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Baseline SVM</th>\n      <td>0.360947</td>\n      <td>0.437276</td>\n      <td>0.395462</td>\n      <td>0.928112</td>\n    </tr>\n    <tr>\n      <th>Baseline SVM Oversampled with SMOTE</th>\n      <td>0.339286</td>\n      <td>0.272401</td>\n      <td>0.302187</td>\n      <td>0.925527</td>\n    </tr>\n    <tr>\n      <th>Baseline SVM Undersampled with Tomek Links</th>\n      <td>0.656250</td>\n      <td>0.225806</td>\n      <td>0.336000</td>\n      <td>0.937993</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(metric_dict, orient='index')"
   ]
  },
  {
   "source": [
    "- The baseline SVM with `class_weight=balanced` has the highest unweighted F1\n",
    "- The undersampled baseline has a lower raw F1, but higher weighted F1.\n",
    "\n",
    "We can take a look at each model's classification report to get a better idea about what's happening."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n     class 0       0.97      0.95      0.96      4678\n     class 1       0.36      0.44      0.40       279\n\n    accuracy                           0.92      4957\n   macro avg       0.66      0.70      0.68      4957\nweighted avg       0.93      0.92      0.93      4957\n\n              precision    recall  f1-score   support\n\n     class 0       0.96      0.99      0.97      4678\n     class 1       0.66      0.23      0.34       279\n\n    accuracy                           0.95      4957\n   macro avg       0.81      0.61      0.65      4957\nweighted avg       0.94      0.95      0.94      4957\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report \n",
    "target_names = ['class 0', 'class 1']\n",
    "# class_weight='balanced' Baseline report\n",
    "print(classification_report(y_test, SVM_test_preds, target_names=target_names))\n",
    "# Undersampled Baseline report\n",
    "print(classification_report(y_test, tomek_logreg_test_preds, target_names=target_names))"
   ]
  },
  {
   "source": [
    "There are some differances. But most noteably, the baseline with `class_weight=balanced` predicts the hate speech (1) class much better than the other model. \n",
    "\n",
    "Therefore, let's stick with that one and grid search to tune its hyperparameters."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Grid Search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the model\n",
    "baseline_model = svm.SVC(degree=3, class_weight='balanced', random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating param_dict\n",
    "param_dict={'C': [1, 10, 100],  \n",
    "              'gamma': [0.1, 0.01, 0.001], \n",
    "              'kernel': ['rbf', 'sigmoid']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate Grid Search CV with F1 metric\n",
    "grid_baseline = GridSearchCV(baseline_model, param_dict, cv=5, scoring='f1', verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[CV] .......... C=1, gamma=0.1, kernel=rbf, score=0.433, total=  24.2s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   24.2s remaining:    0.0s\n",
      "[CV] .......... C=1, gamma=0.1, kernel=rbf, score=0.465, total=  24.0s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   48.2s remaining:    0.0s\n",
      "[CV] .......... C=1, gamma=0.1, kernel=rbf, score=0.446, total=  23.7s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] .......... C=1, gamma=0.1, kernel=rbf, score=0.456, total=  24.0s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] .......... C=1, gamma=0.1, kernel=rbf, score=0.405, total=  23.8s\n",
      "[CV] C=1, gamma=0.1, kernel=sigmoid ..................................\n",
      "[CV] ...... C=1, gamma=0.1, kernel=sigmoid, score=0.439, total=  27.3s\n",
      "[CV] C=1, gamma=0.1, kernel=sigmoid ..................................\n",
      "[CV] ...... C=1, gamma=0.1, kernel=sigmoid, score=0.451, total=  27.4s\n",
      "[CV] C=1, gamma=0.1, kernel=sigmoid ..................................\n",
      "[CV] ...... C=1, gamma=0.1, kernel=sigmoid, score=0.441, total=  28.0s\n",
      "[CV] C=1, gamma=0.1, kernel=sigmoid ..................................\n",
      "[CV] ...... C=1, gamma=0.1, kernel=sigmoid, score=0.450, total=  27.7s\n",
      "[CV] C=1, gamma=0.1, kernel=sigmoid ..................................\n",
      "[CV] ...... C=1, gamma=0.1, kernel=sigmoid, score=0.400, total=  28.1s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ......... C=1, gamma=0.01, kernel=rbf, score=0.360, total=  33.6s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ......... C=1, gamma=0.01, kernel=rbf, score=0.416, total=  33.4s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ......... C=1, gamma=0.01, kernel=rbf, score=0.390, total=  32.6s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ......... C=1, gamma=0.01, kernel=rbf, score=0.376, total=  34.8s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ......... C=1, gamma=0.01, kernel=rbf, score=0.367, total=  33.5s\n",
      "[CV] C=1, gamma=0.01, kernel=sigmoid .................................\n",
      "[CV] ..... C=1, gamma=0.01, kernel=sigmoid, score=0.165, total=  35.1s\n",
      "[CV] C=1, gamma=0.01, kernel=sigmoid .................................\n",
      "[CV] ..... C=1, gamma=0.01, kernel=sigmoid, score=0.164, total=  35.1s\n",
      "[CV] C=1, gamma=0.01, kernel=sigmoid .................................\n",
      "[CV] ..... C=1, gamma=0.01, kernel=sigmoid, score=0.157, total=  33.9s\n",
      "[CV] C=1, gamma=0.01, kernel=sigmoid .................................\n",
      "[CV] ..... C=1, gamma=0.01, kernel=sigmoid, score=0.109, total=  34.3s\n",
      "[CV] C=1, gamma=0.01, kernel=sigmoid .................................\n",
      "[CV] ..... C=1, gamma=0.01, kernel=sigmoid, score=0.185, total=  35.1s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV] ........ C=1, gamma=0.001, kernel=rbf, score=0.034, total=  35.7s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV] ........ C=1, gamma=0.001, kernel=rbf, score=0.110, total=  34.6s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV] ........ C=1, gamma=0.001, kernel=rbf, score=0.110, total=  34.8s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV] ........ C=1, gamma=0.001, kernel=rbf, score=0.110, total=  34.5s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV] ........ C=1, gamma=0.001, kernel=rbf, score=0.110, total=  33.9s\n",
      "[CV] C=1, gamma=0.001, kernel=sigmoid ................................\n",
      "[CV] .... C=1, gamma=0.001, kernel=sigmoid, score=0.000, total=  35.1s\n",
      "[CV] C=1, gamma=0.001, kernel=sigmoid ................................\n",
      "[CV] .... C=1, gamma=0.001, kernel=sigmoid, score=0.110, total=  35.2s\n",
      "[CV] C=1, gamma=0.001, kernel=sigmoid ................................\n",
      "[CV] .... C=1, gamma=0.001, kernel=sigmoid, score=0.110, total=  35.0s\n",
      "[CV] C=1, gamma=0.001, kernel=sigmoid ................................\n",
      "[CV] .... C=1, gamma=0.001, kernel=sigmoid, score=0.110, total=  34.7s\n",
      "[CV] C=1, gamma=0.001, kernel=sigmoid ................................\n",
      "[CV] .... C=1, gamma=0.001, kernel=sigmoid, score=0.110, total=  35.4s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV] ......... C=10, gamma=0.1, kernel=rbf, score=0.312, total=  37.1s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV] ......... C=10, gamma=0.1, kernel=rbf, score=0.267, total=  39.4s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV] ......... C=10, gamma=0.1, kernel=rbf, score=0.253, total=  39.9s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV] ......... C=10, gamma=0.1, kernel=rbf, score=0.319, total=  38.5s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV] ......... C=10, gamma=0.1, kernel=rbf, score=0.282, total=  36.4s\n",
      "[CV] C=10, gamma=0.1, kernel=sigmoid .................................\n",
      "[CV] ..... C=10, gamma=0.1, kernel=sigmoid, score=0.358, total=  31.2s\n",
      "[CV] C=10, gamma=0.1, kernel=sigmoid .................................\n",
      "[CV] ..... C=10, gamma=0.1, kernel=sigmoid, score=0.383, total=  31.7s\n",
      "[CV] C=10, gamma=0.1, kernel=sigmoid .................................\n",
      "[CV] ..... C=10, gamma=0.1, kernel=sigmoid, score=0.331, total=  32.1s\n",
      "[CV] C=10, gamma=0.1, kernel=sigmoid .................................\n",
      "[CV] ..... C=10, gamma=0.1, kernel=sigmoid, score=0.409, total=  32.2s\n",
      "[CV] C=10, gamma=0.1, kernel=sigmoid .................................\n",
      "[CV] ..... C=10, gamma=0.1, kernel=sigmoid, score=0.373, total=  30.1s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV] ........ C=10, gamma=0.01, kernel=rbf, score=0.431, total=  23.8s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV] ........ C=10, gamma=0.01, kernel=rbf, score=0.459, total=  24.0s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV] ........ C=10, gamma=0.01, kernel=rbf, score=0.444, total=  23.9s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV] ........ C=10, gamma=0.01, kernel=rbf, score=0.458, total=  24.2s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV] ........ C=10, gamma=0.01, kernel=rbf, score=0.414, total=  23.1s\n",
      "[CV] C=10, gamma=0.01, kernel=sigmoid ................................\n",
      "[CV] .... C=10, gamma=0.01, kernel=sigmoid, score=0.439, total=  27.2s\n",
      "[CV] C=10, gamma=0.01, kernel=sigmoid ................................\n",
      "[CV] .... C=10, gamma=0.01, kernel=sigmoid, score=0.451, total=  27.8s\n",
      "[CV] C=10, gamma=0.01, kernel=sigmoid ................................\n",
      "[CV] .... C=10, gamma=0.01, kernel=sigmoid, score=0.441, total=  27.5s\n",
      "[CV] C=10, gamma=0.01, kernel=sigmoid ................................\n",
      "[CV] .... C=10, gamma=0.01, kernel=sigmoid, score=0.444, total=  27.7s\n",
      "[CV] C=10, gamma=0.01, kernel=sigmoid ................................\n",
      "[CV] .... C=10, gamma=0.01, kernel=sigmoid, score=0.400, total=  27.2s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] ....... C=10, gamma=0.001, kernel=rbf, score=0.359, total=  34.1s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] ....... C=10, gamma=0.001, kernel=rbf, score=0.419, total=  34.9s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] ....... C=10, gamma=0.001, kernel=rbf, score=0.392, total=  33.8s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] ....... C=10, gamma=0.001, kernel=rbf, score=0.383, total=  33.7s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] ....... C=10, gamma=0.001, kernel=rbf, score=0.367, total=  34.9s\n",
      "[CV] C=10, gamma=0.001, kernel=sigmoid ...............................\n",
      "[CV] ... C=10, gamma=0.001, kernel=sigmoid, score=0.165, total=  35.0s\n",
      "[CV] C=10, gamma=0.001, kernel=sigmoid ...............................\n",
      "[CV] ... C=10, gamma=0.001, kernel=sigmoid, score=0.164, total=  35.4s\n",
      "[CV] C=10, gamma=0.001, kernel=sigmoid ...............................\n",
      "[CV] ... C=10, gamma=0.001, kernel=sigmoid, score=0.157, total=  36.5s\n",
      "[CV] C=10, gamma=0.001, kernel=sigmoid ...............................\n",
      "[CV] ... C=10, gamma=0.001, kernel=sigmoid, score=0.109, total=  35.2s\n",
      "[CV] C=10, gamma=0.001, kernel=sigmoid ...............................\n",
      "[CV] ... C=10, gamma=0.001, kernel=sigmoid, score=0.185, total=  34.7s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ........ C=100, gamma=0.1, kernel=rbf, score=0.256, total=  38.0s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ........ C=100, gamma=0.1, kernel=rbf, score=0.240, total=  38.9s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ........ C=100, gamma=0.1, kernel=rbf, score=0.241, total=  40.8s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ........ C=100, gamma=0.1, kernel=rbf, score=0.279, total=  39.4s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ........ C=100, gamma=0.1, kernel=rbf, score=0.274, total=  37.7s\n",
      "[CV] C=100, gamma=0.1, kernel=sigmoid ................................\n",
      "[CV] .... C=100, gamma=0.1, kernel=sigmoid, score=0.257, total=  38.0s\n",
      "[CV] C=100, gamma=0.1, kernel=sigmoid ................................\n",
      "[CV] .... C=100, gamma=0.1, kernel=sigmoid, score=0.237, total=  40.9s\n",
      "[CV] C=100, gamma=0.1, kernel=sigmoid ................................\n",
      "[CV] .... C=100, gamma=0.1, kernel=sigmoid, score=0.244, total=  40.4s\n",
      "[CV] C=100, gamma=0.1, kernel=sigmoid ................................\n",
      "[CV] .... C=100, gamma=0.1, kernel=sigmoid, score=0.266, total=  36.2s\n",
      "[CV] C=100, gamma=0.1, kernel=sigmoid ................................\n",
      "[CV] .... C=100, gamma=0.1, kernel=sigmoid, score=0.272, total=  35.5s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] ....... C=100, gamma=0.01, kernel=rbf, score=0.317, total=  33.4s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] ....... C=100, gamma=0.01, kernel=rbf, score=0.283, total=  36.2s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] ....... C=100, gamma=0.01, kernel=rbf, score=0.272, total=  35.8s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] ....... C=100, gamma=0.01, kernel=rbf, score=0.326, total=  34.5s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] ....... C=100, gamma=0.01, kernel=rbf, score=0.302, total=  31.6s\n",
      "[CV] C=100, gamma=0.01, kernel=sigmoid ...............................\n",
      "[CV] ... C=100, gamma=0.01, kernel=sigmoid, score=0.354, total=  29.0s\n",
      "[CV] C=100, gamma=0.01, kernel=sigmoid ...............................\n",
      "[CV] ... C=100, gamma=0.01, kernel=sigmoid, score=0.374, total=  31.3s\n",
      "[CV] C=100, gamma=0.01, kernel=sigmoid ...............................\n",
      "[CV] ... C=100, gamma=0.01, kernel=sigmoid, score=0.331, total=  33.3s\n",
      "[CV] C=100, gamma=0.01, kernel=sigmoid ...............................\n",
      "[CV] ... C=100, gamma=0.01, kernel=sigmoid, score=0.410, total=  31.5s\n",
      "[CV] C=100, gamma=0.01, kernel=sigmoid ...............................\n",
      "[CV] ... C=100, gamma=0.01, kernel=sigmoid, score=0.374, total=  29.0s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ...... C=100, gamma=0.001, kernel=rbf, score=0.430, total=  22.9s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ...... C=100, gamma=0.001, kernel=rbf, score=0.458, total=  23.5s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ...... C=100, gamma=0.001, kernel=rbf, score=0.444, total=  23.5s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ...... C=100, gamma=0.001, kernel=rbf, score=0.457, total=  23.2s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ...... C=100, gamma=0.001, kernel=rbf, score=0.412, total=  23.6s\n",
      "[CV] C=100, gamma=0.001, kernel=sigmoid ..............................\n",
      "[CV] .. C=100, gamma=0.001, kernel=sigmoid, score=0.439, total=  27.9s\n",
      "[CV] C=100, gamma=0.001, kernel=sigmoid ..............................\n",
      "[CV] .. C=100, gamma=0.001, kernel=sigmoid, score=0.451, total=  27.7s\n",
      "[CV] C=100, gamma=0.001, kernel=sigmoid ..............................\n",
      "[CV] .. C=100, gamma=0.001, kernel=sigmoid, score=0.441, total=  28.0s\n",
      "[CV] C=100, gamma=0.001, kernel=sigmoid ..............................\n",
      "[CV] .. C=100, gamma=0.001, kernel=sigmoid, score=0.444, total=  28.2s\n",
      "[CV] C=100, gamma=0.001, kernel=sigmoid ..............................\n",
      "[CV] .. C=100, gamma=0.001, kernel=sigmoid, score=0.400, total=  27.9s\n",
      "[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed: 47.8min finished\n",
      "CPU times: user 47min 26s, sys: 38.6 s, total: 48min 5s\n",
      "Wall time: 48min 31s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(class_weight='balanced', random_state=20),\n",
       "             param_grid={'C': [1, 10, 100], 'gamma': [0.1, 0.01, 0.001],\n",
       "                         'kernel': ['rbf', 'sigmoid']},\n",
       "             scoring='f1', verbose=3)"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "%%time\n",
    "# fit the grid search to our data\n",
    "grid_baseline.fit(tfidf_data_train, y_train)\n",
    "\n",
    "# this cell takes 48 minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "F1 Score: 0.44115028504856885\nBest Hyperparameters: {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\nModel object with best parameters: \nSVC(C=1, class_weight='balanced', gamma=0.1, random_state=20)\n"
     ]
    }
   ],
   "source": [
    "# generate score with .best_score_ and hyperparemeters with .best_params_\n",
    "print('F1 Score:', grid_baseline.best_score_)\n",
    "print('Best Hyperparameters:', grid_baseline.best_params_)\n",
    "print('Model object with best parameters: ')\n",
    "print(grid_baseline.best_estimator_)"
   ]
  },
  {
   "source": [
    "The grid search found that the best hyperparameters are `{'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}`.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tuned SVM Model Predictions\nF1 on train set: 0.7077399380804954\nF1 on test set: 0.38513513513513514\n"
     ]
    }
   ],
   "source": [
    "# Predict the response for test dataset\n",
    "grid_base_y_pred_train = grid_baseline.best_estimator_.predict(tfidf_data_train)\n",
    "\n",
    "# predict the training set\n",
    "grid_base_y_pred_test = grid_baseline.best_estimator_.predict(tfidf_data_test)\n",
    "\n",
    "# Model F1, how often is the classifier correct?\n",
    "print('Tuned SVM Model Predictions')\n",
    "print(\"F1 on train set:\",metrics.f1_score(y_train, grid_base_y_pred_train))\n",
    "print(\"F1 on test set:\",metrics.f1_score(y_test, grid_base_y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting variables for evaluation metrics\n",
    "grid_precision = precision_score(y_test, grid_base_y_pred_test)\n",
    "grid_recall = recall_score(y_test, grid_base_y_pred_test)\n",
    "grid_f1_score = f1_score(y_test, grid_base_y_pred_test)\n",
    "grid_weighted_f1_score = f1_score(y_test, grid_base_y_pred_test, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding these metrics to evaluation metric dict\n",
    "metric_dict['Grid Search SVM'] = {'precision': grid_precision, 'recall': grid_recall, 'f1_score': grid_f1_score, 'weighted_f1': grid_weighted_f1_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                            precision    recall  f1_score  \\\n",
       "Baseline SVM                                 0.360947  0.437276  0.395462   \n",
       "Baseline SVM Oversampled with SMOTE          0.339286  0.272401  0.302187   \n",
       "Baseline SVM Undersampled with Tomek Links   0.656250  0.225806  0.336000   \n",
       "Grid Search SVM                              0.280788  0.612903  0.385135   \n",
       "\n",
       "                                            weighted_f1  \n",
       "Baseline SVM                                   0.928112  \n",
       "Baseline SVM Oversampled with SMOTE            0.925527  \n",
       "Baseline SVM Undersampled with Tomek Links     0.937993  \n",
       "Grid Search SVM                                0.908306  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>f1_score</th>\n      <th>weighted_f1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Baseline SVM</th>\n      <td>0.360947</td>\n      <td>0.437276</td>\n      <td>0.395462</td>\n      <td>0.928112</td>\n    </tr>\n    <tr>\n      <th>Baseline SVM Oversampled with SMOTE</th>\n      <td>0.339286</td>\n      <td>0.272401</td>\n      <td>0.302187</td>\n      <td>0.925527</td>\n    </tr>\n    <tr>\n      <th>Baseline SVM Undersampled with Tomek Links</th>\n      <td>0.656250</td>\n      <td>0.225806</td>\n      <td>0.336000</td>\n      <td>0.937993</td>\n    </tr>\n    <tr>\n      <th>Grid Search SVM</th>\n      <td>0.280788</td>\n      <td>0.612903</td>\n      <td>0.385135</td>\n      <td>0.908306</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "# comparing with other models\n",
    "pd.DataFrame.from_dict(metric_dict, orient='index')"
   ]
  },
  {
   "source": [
    "Unfortunately, the model with grid searched hyperparameters didn't perform better than the baseline."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}