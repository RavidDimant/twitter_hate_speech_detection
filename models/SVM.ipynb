{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Support Vector Machine (SVM) Modeling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "from sklearn.feature_extraction import text \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn import metrics, model_selection, svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, plot_confusion_matrix, roc_curve, auc, classification_report\n",
    "import pickle"
   ]
  },
  {
   "source": [
    "## Importing X and y from `nlp_preprocessing.ipynb`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lem = pickle.load(open('../pickle/X_lem.pkl', 'rb'))\n",
    "y_lem = pd.read_pickle('../pickle/y_lem.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up stop words\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "source": [
    "## Train-Test Split & Vectorize"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_lem, y_lem, test_size=0.20, random_state=15)\n",
    "\n",
    "# using tf_idf vectorizor\n",
    "tfidf = TfidfVectorizer(stop_words= stop_words, ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse matrix format with 265K stored elements\n",
    "tfidf_data_train = tfidf.fit_transform(X_train)\n",
    "tfidf_data_test = tfidf.transform(X_test)"
   ]
  },
  {
   "source": [
    "## SVM Baseline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_baseline = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto', random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the classifier\n",
    "SVM_baseline.fit(tfidf_data_train, y_train)\n",
    "# predict the labels on validation dataset\n",
    "SVM_test_preds = SVM_baseline.predict(tfidf_data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_precision = precision_score(y_test, SVM_test_preds)\n",
    "baseline_recall = recall_score(y_test, SVM_test_preds)\n",
    "baseline_f1_score = f1_score(y_test, SVM_test_preds)\n",
    "baseline_weighted_f1_score = f1_score(y_test, SVM_test_preds, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Metrics for SVM Baseline with Lemmatization & TF-IDF Vectorization\nPrecision: 0.6739\nRecall: 0.2222\nF1 Score: 0.3342\nWeighted F1 Score: 0.9381\n"
     ]
    }
   ],
   "source": [
    "# printing evaluation metrics up to 4th decimal place\n",
    "print('Testing Metrics for SVM Baseline with Lemmatization & TF-IDF Vectorization')\n",
    "print('Precision: {:.4}'.format(baseline_precision))\n",
    "print('Recall: {:.4}'.format(baseline_recall))\n",
    "print('F1 Score: {:.4}'.format(baseline_f1_score))\n",
    "print('Weighted F1 Score: {:.4}'.format(baseline_weighted_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dictionary with all metrics\n",
    "metric_dict = {}\n",
    "metric_dict['Baseline SVM'] = {'precision': baseline_precision, 'recall': baseline_recall, 'f1_score': baseline_f1_score, 'weighted_f1': baseline_weighted_f1_score}"
   ]
  },
  {
   "source": [
    "## Baseline with SMOTE\n",
    "Used to over-sample the minority class (hate speech)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=35)\n",
    "smote_X_train, smote_y_train = sm.fit_sample(tfidf_data_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto', random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 3min 52s, sys: 2.39 s, total: 3min 54s\nWall time: 3min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# this cell takes about 4 minutes to run\n",
    "smote_SVM.fit(smote_X_train, smote_y_train)\n",
    "smote_SVM_test_preds = smote_SVM.predict(tfidf_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_precision = precision_score(y_test, smote_SVM_test_preds)\n",
    "smote_recall = recall_score(y_test, smote_SVM_test_preds)\n",
    "smote_f1_score = f1_score(y_test, smote_SVM_test_preds)\n",
    "smote_weighted_f1_score = f1_score(y_test, smote_SVM_test_preds, average='weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Metrics for Oversampled SVM Baseline with Lemmatization\nPrecision: 0.3393\nRecall: 0.2724\nF1 Score: 0.3022\nWeighted F1 Score: 0.9255\n"
     ]
    }
   ],
   "source": [
    "# printing evaluation metrics up to 4th decimal place\n",
    "print('Testing Metrics for Oversampled SVM Baseline with Lemmatization')\n",
    "print('Precision: {:.4}'.format(smote_precision))\n",
    "print('Recall: {:.4}'.format(smote_recall))\n",
    "print('F1 Score: {:.4}'.format(smote_f1_score))\n",
    "print('Weighted F1 Score: {:.4}'.format(smote_weighted_f1_score))"
   ]
  },
  {
   "source": [
    "Looks like SMOTE actually decreased the F1, which also happened with Logistic Regression."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding these metrics to evaluation metric dict\n",
    "metric_dict['Baseline SVM Oversampled with SMOTE'] = {'precision': smote_precision, 'recall': smote_recall, 'f1_score': smote_f1_score, 'weighted_f1': smote_weighted_f1_score}"
   ]
  },
  {
   "source": [
    "## Baseline with Tomek Links\n",
    "Used to under-sample the majority class (not hate speech)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Resampled dataset shape Counter({0: 18627, 1: 1151})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from imblearn.under_sampling import TomekLinks # doctest: +NORMALIZE_WHITESPACE\n",
    "\n",
    "tl = TomekLinks()\n",
    "tomek_X_train, tomek_y_train = tl.fit_resample(tfidf_data_train, y_train)\n",
    "print('Resampled dataset shape %s' % Counter(tomek_y_train))"
   ]
  },
  {
   "source": [
    "Only removed 48 values from the majority class."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomek_SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto', random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 42.9 s, sys: 978 ms, total: 43.9 s\nWall time: 44.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# this cell takes 42 seconds to run\n",
    "tomek_SVM.fit(tomek_X_train, tomek_y_train)\n",
    "tomek_logreg_test_preds = tomek_SVM.predict(tfidf_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomek_precision = precision_score(y_test, tomek_logreg_test_preds)\n",
    "tomek_recall = recall_score(y_test, tomek_logreg_test_preds)\n",
    "tomek_f1_score = f1_score(y_test, tomek_logreg_test_preds)\n",
    "tomek_weighted_f1_score = f1_score(y_test, tomek_logreg_test_preds, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Metrics for Undersampled SVM Baseline with Lemmatization\nPrecision: 0.6562\nRecall: 0.2258\nF1 Score: 0.336\nF1 Score: 0.938\n"
     ]
    }
   ],
   "source": [
    "# printing evaluation metrics up to 4th decimal place\n",
    "print('Testing Metrics for Undersampled SVM Baseline with Lemmatization')\n",
    "print('Precision: {:.4}'.format(tomek_precision))\n",
    "print('Recall: {:.4}'.format(tomek_recall))\n",
    "print('F1 Score: {:.4}'.format(tomek_f1_score))\n",
    "print('F1 Score: {:.4}'.format(tomek_weighted_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding these metrics to evaluation metric dict\n",
    "metric_dict['Baseline SVM Undersampled with Tomek Links'] = {'precision': tomek_precision, 'recall': tomek_recall, 'f1_score': tomek_f1_score, 'weighted_f1': tomek_weighted_f1_score}"
   ]
  },
  {
   "source": [
    "## Metrics for All Baselines"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                            precision    recall  f1_score  \\\n",
       "Baseline SVM                                 0.673913  0.222222  0.334232   \n",
       "Baseline SVM Oversampled with SMOTE          0.339286  0.272401  0.302187   \n",
       "Baseline SVM Undersampled with Tomek Links   0.656250  0.225806  0.336000   \n",
       "\n",
       "                                            weighted_f1  \n",
       "Baseline SVM                                   0.938102  \n",
       "Baseline SVM Oversampled with SMOTE            0.925527  \n",
       "Baseline SVM Undersampled with Tomek Links     0.937993  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>f1_score</th>\n      <th>weighted_f1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Baseline SVM</th>\n      <td>0.673913</td>\n      <td>0.222222</td>\n      <td>0.334232</td>\n      <td>0.938102</td>\n    </tr>\n    <tr>\n      <th>Baseline SVM Oversampled with SMOTE</th>\n      <td>0.339286</td>\n      <td>0.272401</td>\n      <td>0.302187</td>\n      <td>0.925527</td>\n    </tr>\n    <tr>\n      <th>Baseline SVM Undersampled with Tomek Links</th>\n      <td>0.656250</td>\n      <td>0.225806</td>\n      <td>0.336000</td>\n      <td>0.937993</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(metric_dict, orient='index')"
   ]
  },
  {
   "source": [
    "There's a tiny bit of improvement with the undersampled baseline's unweighted F1.\n",
    "\n",
    "First let's grid search on the baseline, and then apply that to the Tomek Links undersampled data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Grid Search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}