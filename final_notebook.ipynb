{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Twitter Hate Speech Detection\n",
    "\n",
    "**Please note:** Because of the subject matter of this project, this notebook contains uncensored offensive language from the dataset.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Overview"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Business Problem\n",
    "\n",
    "Human content moderation exploits people by consistently traumatizing and underpaying them. In 2019, an [article](https://www.theverge.com/2019/6/19/18681845/facebook-moderator-interviews-video-trauma-ptsd-cognizant-tampa) on The Verge exposed the extensive list of horrific working conditions that employees faced at Cognizant, which was Facebook’s primary moderation contractor. Unfortunately, every major tech company, including Twitter, uses human moderators to some extent, both domestically and overseas.\n",
    "\n",
    "Hate speech is defined as abusive or threatening speech that expresses prejudice against a particular group, especially on the basis of race, religion or sexual orientation. Usually, the difference between hate speech and offensive language comes down to subtle context or diction. \n",
    "\n",
    "Any company with an online forum where users post content could benefit from automating as much as the moderation process as possible. Ultimately, human content moderation is not only detrimental to workers, but also presents a liability to companies that use them."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Data & Methods\n",
    "\n",
    "The dataset for this capstone project was sourced from a study called Automated Hate Speech Detection and the Problem of Offensive Language  conducted by Thomas Davidson and a team at Cornell University in 2017. The GitHub repository can be found [here](https://github.com/t-davidson/hate-speech-and-offensive-language). The dataset is provided as a `.csv` file with 24,802 text posts from Twitter where 6% of the tweets were labeled as hate speech. \n",
    "\n",
    "Since content moderation is so subjective, the labels on this dataset were crowdsourced and determined by majority-rules. The “class” column labels each tweet as 0 for hate speech, 1 for offensive language or 2 for neither. In order to create a different project and adapt the data to my specific business context, I will be treating the data as a binary classification problem. \n",
    "\n",
    "Therefore, the final model will be **predicting whether a tweet is hate speech or not.** To prepare the data for this, I will be manually replacing existing 1 and 2 values as 0, and replacing 0 as 1 to indicate hate speech."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Data Understanding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1. What are the linguistic differences between hate speech and offensive language?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![img1](./visualizations/label_word_count_y.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Linguistically, it's important to note that the difference between hate speech and offensive language often comes down to how it targets marginalized communities, often in threatening ways.\n",
    "\n",
    "Although these graphs have very similar frequently occurring words, there are a few that stand out. For instance, we can notice from this figure that Hate Speech typically contains the N-word with the hard 'R'. **The use of this slur could indicate malicious intent, which goes beyond possibly using the word as slang.**\n",
    "\n",
    "Examples like that one demonstrate the nuances of English slang and the fine line between Hate Speech and offensive language. **Because of the similarities of each label’s vocabulary, it could be difficult for machine learning algorithms to differentiate between them and determine what counts as Hate Speech.**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 2. What are the most popular hashtags of each tweet type?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![img2](./visualizations/censored_top_hashtags.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "From these word clouds, we can see some more parallels and differences between what is classified as hate speech or not. For instance, #tcot stands for \"Top Conservatives On Twitter” and it appears in both groups. However, #teabagger, which refers to those who identify with the Tea Party, that is primarily (but not exclusively) associated with the Repubclican Party, appears in only the “Not Hate Speech” cloud. Both hashtags are used among Alt-Right communities.\n",
    "\n",
    "Additionally, the #r**skins hashtag appears in only the Not Hate Speech cloud. This was the former name of the Washington NFL team. Knowing the context, we know that hashtag could certainly include text that constitutes as hate speech. WIth this, and other hashtags that appear in the “Not Hate Speech” cloud, we can clearly see the very slight differences between the two labels.\n",
    "\n",
    "Besides that, others are simply pop culture references, such as #Scandal the TV show or #vote5sos referring to the boy band. It’s interesting that those contain a lot of offensive language, probably from fan reactions and community conflicts. Ultimately, we can recommend that **Twitter should closely monitor those top hashtags for potential posts containing hate speech** or even regular offensive language.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3. What is the overall polarity of the tweets?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![img3](./visualizations/compound_polarity_score.png)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The compound polarity score is a metric that calculates the sum of all the [lexicon ratings](https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt) which have been normalized between -1 and +1. With -1 being extreme negative and +1 being extreme positive. **This score encompasses the overall sentiment of this corpus.**\n",
    "\n",
    "- Hate Speech tweets on average have a compound score of -0.363\n",
    "- Non Hate Speech tweets on average have a compound score of -0.263\n",
    "\n",
    "According to this metric, both classes of tweets have pretty negative sentiments because their normalized compound scores are less than -0.05. \n",
    "\n",
    "Additionally from this graph, we can see that tweets classified as Hate Speech are especially negative. This further emphasizes how slim the difference between the two labels are. Although both classes contain negative and offensive language, Hate Speech is much more negative on average.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![img3](./visualizations/avg_polarity_by_tweet_type.png)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "To reiterate, this graph shows the average polarity scores for each label. We can see that a majority were scored as neutral. However, of those that were scored as negative, it seems like \"Not Hate Speech\" had more on average."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Class Imbalance\n",
    "\n",
    "The main roadblock of this dataset is the extreme class imbalance. We can see that only 5.77% of the data is labeled as hate speech. This could present challenges during the modeling process."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![imbalance](./visualizations/cleaned_class_imbalance.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "import pickle\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from sklearn.feature_extraction import text \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# modeling libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics, model_selection, svm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, plot_confusion_matrix, roc_curve, auc, classification_report"
   ]
  },
  {
   "source": [
    "# Preprocessing Text Data\n",
    "\n",
    "The original data from `twitter_data.csv` was cleaned using RegEx in the `data_cleaning.ipynb` notebook. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in clean_df\n",
    "clean_df = pd.read_pickle('./pickle/clean_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   total_votes  hate_speech_votes  other_votes  label  \\\n",
       "0            3                  0            3      0   \n",
       "1            3                  0            3      0   \n",
       "2            3                  0            3      0   \n",
       "\n",
       "                                               tweet  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...   \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...   \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...   \n",
       "\n",
       "                                        clean_tweets  \n",
       "0     as a woman you shouldnt complain about clea...  \n",
       "1     boy dats coldtyga dwn bad for cuffin dat ho...  \n",
       "2     dawg   you ever fuck a bitch and she sta to...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>total_votes</th>\n      <th>hate_speech_votes</th>\n      <th>other_votes</th>\n      <th>label</th>\n      <th>tweet</th>\n      <th>clean_tweets</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n      <td>as a woman you shouldnt complain about clea...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n      <td>boy dats coldtyga dwn bad for cuffin dat ho...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n      <td>dawg   you ever fuck a bitch and she sta to...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "clean_df.head(3)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Tokenizing & Removing Stop Words\n",
    "\n",
    "When working with text data, one of the first steps is to remove stop words from the corpus. Although text would be gramatically incorrect without these stop words, they provide little value to models and typically hinder performace.\n",
    "\n",
    "We can use NLTK's built-in library of stop words to remove them in a tokenizing function.\n",
    "\n",
    "Additionally, we tokenize text data so that machine learning algorithms can understand it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# assigning variables to essential columns\n",
    "data = clean_df['clean_tweets']\n",
    "target = clean_df['label']"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 53,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting NLTK stop words as `stop_words`\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to tokeenize text in each column and remove stop words\n",
    "def process_tweet(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stopwords_removed = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
    "    return stopwords_removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the above function to our data/features \n",
    "processed_data = list(map(process_tweet, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20277"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "# getting count of all unique words in the corpus\n",
    "total_vocab = set()\n",
    "for comment in processed_data:\n",
    "    total_vocab.update(comment)\n",
    "len(total_vocab)"
   ]
  },
  {
   "source": [
    "We can see that the corpus has a vocabulary of 20,277 unique words."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Top Words in Corpus"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('bitch', 8227),\n",
       " ('bitches', 3083),\n",
       " ('like', 2766),\n",
       " ('hoes', 2368),\n",
       " ('pussy', 2099),\n",
       " ('im', 2061),\n",
       " ('hoe', 1906),\n",
       " ('dont', 1749),\n",
       " ('got', 1597),\n",
       " ('ass', 1570),\n",
       " ('get', 1428),\n",
       " ('fuck', 1411),\n",
       " ('u', 1280),\n",
       " ('shit', 1262),\n",
       " ('nigga', 1198),\n",
       " ('aint', 1158),\n",
       " ('trash', 1142),\n",
       " ('lol', 1074),\n",
       " ('know', 806),\n",
       " ('niggas', 791)]"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "# morphing `processed_data` into a readable list\n",
    "flat_filtered = [item for sublist in processed_data for item in sublist]\n",
    "# getting frequency distribution\n",
    "clean_corpus_freqdist = FreqDist(flat_filtered)\n",
    "# top 20 words in cleaned corpus\n",
    "clean_corpus_freqdist.most_common(20)"
   ]
  },
  {
   "source": [
    "These top words are mostly offensive terms, and some other slang words that the NLTK stop words removal function didn't pick up."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Lemmatization\n",
    "\n",
    "This last method reduces each word into a linguistically valid **lemma**, or root word. It does this through linguistic mappings, using the WordNet lexical database.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list with all lemmatized outputs\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "lemmatized_output = []\n",
    "\n",
    "for listy in processed_data:\n",
    "    lemmed = ' '.join([lemmatizer.lemmatize(w) for w in listy])\n",
    "    lemmatized_output.append(lemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lem = lemmatized_output\n",
    "y_lem = target"
   ]
  },
  {
   "source": [
    "Now `X_lem` and `y_lem` are contain cleaned text and the original labels, ready to be used for modeling."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Feature Engineering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Train-Test Split"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_lem, y_lem, test_size=0.20, random_state=15)"
   ]
  },
  {
   "source": [
    "## TF-IDF Vectorization\n",
    "\n",
    "Before tokenized text data can be fed into machine learning models, they must be transformed into numerical feature vectors. First, we’ll be trying one of the most popular methods, TF-IDF Vectorization.\n",
    "\n",
    "This is an acronym than stands for “Term Frequency — Inverse Document” Frequency which are the components of the resulting scores assigned to each word.\n",
    "- Term Frequency: This summarizes how often a given word appears within a document.\n",
    "- Inverse Document Frequency: This down scales words that appear a lot across documents.\n",
    "\n",
    "Without going into the math, TF-IDF are word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but not across other documents."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using tf_idf vectorizor\n",
    "tfidf = TfidfVectorizer(stop_words= stop_words, ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse matrix format with 265K stored elements\n",
    "tfidf_data_train = tfidf.fit_transform(X_train)\n",
    "tfidf_data_test = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Average Number of Non-Zero Elements in Vectorized Articles: 13.364420457984465\nPercentage of columns containing ZERO: 0.9998720483637183\n"
     ]
    }
   ],
   "source": [
    "# taking a quick look of the non zero elements\n",
    "non_zero_cols = tfidf_data_train.nnz / float(tfidf_data_train.shape[0])\n",
    "print(\"Average Number of Non-Zero Elements in Vectorized Articles: {}\".format(non_zero_cols))\n",
    "\n",
    "percent_sparse = 1 - (non_zero_cols / float(tfidf_data_train.shape[1]))\n",
    "print('Percentage of columns containing ZERO: {}'.format(percent_sparse))"
   ]
  },
  {
   "source": [
    "99.9% of the columns contain a zero, meaning that's a very **sparse matrix** with 265K columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Baseline Modeling\n",
    "\n",
    "We use the evaluation metrics Precision, Recall and F1 score to determine success. However, F1 is valued as the “most important” metric because by finding an equal balance between Precision and Recall, it's useful for data with high class imbalance.\n",
    "\n",
    "Overall, we want as much hate speech to flagged as possible and so that it can be efficiently removed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to print evaluation metrics\n",
    "def evaluation(precision, recall, f1, f1_weighted):\n",
    "    print('Testing Evaluation Metrics:')\n",
    "    print('Precision: {:.4}'.format(precision))\n",
    "    print('Recall: {:.4}'.format(recall))\n",
    "    print('F1 Score: {:.4}'.format(f1))\n",
    "    print('Weighted F1 Score: {:.4}'.format(f1_weighted))"
   ]
  },
  {
   "source": [
    "## Baseline Random Forest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_baseline = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=0)\n",
    "# adding class_weight='balanced' increased accuracy & precision but decreased F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 42.2 s, sys: 366 ms, total: 42.5 s\nWall time: 43.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# this cell takes 38 seconds to run\n",
    "rf_baseline.fit(tfidf_data_train, y_train)\n",
    "rf_test_preds = rf_baseline.predict(tfidf_data_test)"
   ]
  },
  {
   "source": [
    "rf_precision = precision_score(y_test, rf_test_preds)\n",
    "rf_recall = recall_score(y_test, rf_test_preds)\n",
    "rf_f1_score = f1_score(y_test, rf_test_preds)\n",
    "rf_f1_weighted = f1_score(y_test, rf_test_preds, average='weighted')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 68,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Evaluation Metrics:\nPrecision: 0.4128\nRecall: 0.1613\nF1 Score: 0.232\nWeighted F1 Score: 0.9272\n"
     ]
    }
   ],
   "source": [
    "evaluation(rf_precision, rf_recall, rf_f1_score, rf_f1_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dictionary to store all metrics\n",
    "metric_dict = {}\n",
    "# adding scores to metric_dict\n",
    "metric_dict['Baseline Random Forest'] = {'precision': rf_precision, 'recall': rf_recall, 'f1_score': rf_f1_score, 'weighted_f1': rf_f1_weighted}"
   ]
  },
  {
   "source": [
    "## Baseline Logistic Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_baseline = LogisticRegression(penalty='l2', class_weight='balanced', random_state=20)\n",
    "# class_weight='balanced' actually didn't impact the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 2.96 s, sys: 216 ms, total: 3.18 s\nWall time: 1.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# this cell takes 3 seconds to run\n",
    "logreg_baseline.fit(tfidf_data_train, y_train)\n",
    "logreg_test_preds = logreg_baseline.predict(tfidf_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_precision = precision_score(y_test, logreg_test_preds)\n",
    "logreg_recall = recall_score(y_test, logreg_test_preds)\n",
    "logreg_f1_score = f1_score(y_test, logreg_test_preds)\n",
    "logreg_f1_weighted = f1_score(y_test, logreg_test_preds, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Evaluation Metrics:\nPrecision: 0.2939\nRecall: 0.5699\nF1 Score: 0.3878\nWeighted F1 Score: 0.9134\n"
     ]
    }
   ],
   "source": [
    "evaluation(logreg_precision, logreg_recall, logreg_f1_score, logreg_f1_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding scores to metric_dict\n",
    "metric_dict['Baseline Logisitic Regression'] = {'precision': logreg_precision, 'recall': logreg_recall, 'f1_score': logreg_f1_score, 'weighted_f1': logreg_f1_weighted}"
   ]
  },
  {
   "source": [
    "## Baseline Naive Bayes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_bayes = MultinomialNB(alpha = .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 11 ms, sys: 5.2 ms, total: 16.2 ms\nWall time: 15.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# this cell takes 11 seconds to run\n",
    "baseline_bayes.fit(tfidf_data_train, y_train)\n",
    "bayes_test_preds = baseline_bayes.predict(tfidf_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_precision = precision_score(y_test, bayes_test_preds)\n",
    "bayes_recall = recall_score(y_test, bayes_test_preds)\n",
    "bayes_f1_score = f1_score(y_test, bayes_test_preds)\n",
    "bayes_f1_weighted = f1_score(y_test, bayes_test_preds, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Evaluation Metrics:\nPrecision: 0.4118\nRecall: 0.1254\nF1 Score: 0.1923\nWeighted F1 Score: 0.9255\n"
     ]
    }
   ],
   "source": [
    "evaluation(bayes_precision, bayes_recall, bayes_f1_score, bayes_f1_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding scores to metric_dict\n",
    "metric_dict['Baseline Naive Bayes'] = {'precision': bayes_precision, 'recall': bayes_recall, 'f1_score': bayes_f1_score, 'weighted_f1': bayes_f1_weighted}"
   ]
  },
  {
   "source": [
    "## Baseline Support Vector Machine (SVM)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_baseline = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto', class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 51.3 s, sys: 874 ms, total: 52.2 s\nWall time: 52.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# this cell takes about 1 minute to run\n",
    "# fit the training dataset on the classifier\n",
    "SVM_baseline.fit(tfidf_data_train, y_train)\n",
    "# predict the labels on validation dataset\n",
    "SVM_test_preds = SVM_baseline.predict(tfidf_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_precision = precision_score(y_test, SVM_test_preds)\n",
    "SVM_recall = recall_score(y_test, SVM_test_preds)\n",
    "SVM_f1_score = f1_score(y_test, SVM_test_preds)\n",
    "SVM_f1_weighted = f1_score(y_test, SVM_test_preds, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Evaluation Metrics:\nPrecision: 0.3609\nRecall: 0.4373\nF1 Score: 0.3955\nWeighted F1 Score: 0.9281\n"
     ]
    }
   ],
   "source": [
    "evaluation(SVM_precision, SVM_recall, SVM_f1_score, SVM_f1_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding scores to metric_dict\n",
    "metric_dict['Baseline SVM'] = {'precision': SVM_precision, 'recall': SVM_recall, 'f1_score': SVM_f1_score, 'weighted_f1': SVM_f1_weighted}"
   ]
  },
  {
   "source": [
    "## Evaluation Metrics for All Baseline Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                               precision    recall  f1_score  weighted_f1\n",
       "Baseline Random Forest          0.412844  0.161290  0.231959     0.927249\n",
       "Baseline Logisitic Regression   0.293900  0.569892  0.387805     0.913449\n",
       "Baseline Naive Bayes            0.411765  0.125448  0.192308     0.925487\n",
       "Baseline SVM                    0.360947  0.437276  0.395462     0.928112"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>f1_score</th>\n      <th>weighted_f1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Baseline Random Forest</th>\n      <td>0.412844</td>\n      <td>0.161290</td>\n      <td>0.231959</td>\n      <td>0.927249</td>\n    </tr>\n    <tr>\n      <th>Baseline Logisitic Regression</th>\n      <td>0.293900</td>\n      <td>0.569892</td>\n      <td>0.387805</td>\n      <td>0.913449</td>\n    </tr>\n    <tr>\n      <th>Baseline Naive Bayes</th>\n      <td>0.411765</td>\n      <td>0.125448</td>\n      <td>0.192308</td>\n      <td>0.925487</td>\n    </tr>\n    <tr>\n      <th>Baseline SVM</th>\n      <td>0.360947</td>\n      <td>0.437276</td>\n      <td>0.395462</td>\n      <td>0.928112</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(metric_dict, orient='index')"
   ]
  },
  {
   "source": [
    "Overall, the SVM model performed the best across unweighted and weighted F1. However, there is still work to be done to bring that score up higher. Next we'll try improving that model with grid search."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### SVM class imabalance methods"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### SVM Grid Search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Doc2Vec\n",
    "instead of TF-IDF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Evaluating All Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Pipeline Final Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Training model on full dataset\n",
    "using .fit_transform(X_lem)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Conclusion"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Next Steps"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}