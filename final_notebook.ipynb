{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Twitter Hate Speech Detection\n",
    "\n",
    "**Please note:** Because of the subject matter of this project, this notebook contains uncensored offensive language from the dataset.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Overview"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Business Problem\n",
    "\n",
    "Human content moderation exploits people by consistently traumatizing and underpaying them. In 2019, an [article](https://www.theverge.com/2019/6/19/18681845/facebook-moderator-interviews-video-trauma-ptsd-cognizant-tampa) on The Verge exposed the extensive list of horrific working conditions that employees faced at Cognizant, which was Facebook’s primary moderation contractor. Unfortunately, every major tech company, including Twitter, uses human moderators to some extent, both domestically and overseas.\n",
    "\n",
    "Hate speech is defined as abusive or threatening speech that expresses prejudice against a particular group, especially on the basis of race, religion or sexual orientation. Usually, the difference between hate speech and offensive language comes down to subtle context or diction. \n",
    "\n",
    "Any company with an online forum where users post content could benefit from automating as much as the moderation process as possible. Ultimately, human content moderation is not only detrimental to workers, but also presents a liability to companies that use them."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Data & Methods\n",
    "\n",
    "The dataset for this capstone project was sourced from a study called Automated Hate Speech Detection and the Problem of Offensive Language  conducted by Thomas Davidson and a team at Cornell University in 2017. The GitHub repository can be found [here](https://github.com/t-davidson/hate-speech-and-offensive-language). The dataset is provided as a `.csv` file with 24,802 text posts from Twitter where 6% of the tweets were labeled as hate speech. \n",
    "\n",
    "Since content moderation is so subjective, the labels on this dataset were crowdsourced and determined by majority-rules. The “class” column labels each tweet as 0 for hate speech, 1 for offensive language or 2 for neither. In order to create a different project and adapt the data to my specific business context, I will be treating the data as a binary classification problem. \n",
    "\n",
    "Therefore, the final model will be **predicting whether a tweet is hate speech or not.** To prepare the data for this, I will be manually replacing existing 1 and 2 values as 0, and replacing 0 as 1 to indicate hate speech."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Data Understanding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1. What are the linguistic differences between hate speech and offensive language?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![img1](./visualizations/label_word_count_y.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Linguistically, it's important to note that the difference between hate speech and offensive language often comes down to how it targets marginalized communities, often in threatening ways.\n",
    "\n",
    "Although these graphs have very similar frequently occurring words, there are a few that stand out. For instance, we can notice from this figure that Hate Speech typically contains the N-word with the hard 'R'. **The use of this slur could indicate malicious intent, which goes beyond possibly using the word as slang.**\n",
    "\n",
    "Examples like that one demonstrate the nuances of English slang and the fine line between Hate Speech and offensive language. **Because of the similarities of each label’s vocabulary, it could be difficult for machine learning algorithms to differentiate between them and determine what counts as Hate Speech.**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 2. What are the most popular hashtags of each tweet type?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![img2](./visualizations/censored_top_hashtags.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "From these word clouds, we can see some more parallels and differences between what is classified as hate speech or not. For instance, #tcot stands for \"Top Conservatives On Twitter” and it appears in both groups. However, #teabagger, which refers to those who identify with the Tea Party, that is primarily (but not exclusively) associated with the Repubclican Party, appears in only the “Not Hate Speech” cloud. Both hashtags are used among Alt-Right communities.\n",
    "\n",
    "Additionally, the #r**skins hashtag appears in only the Not Hate Speech cloud. This was the former name of the Washington NFL team. Knowing the context, we know that hashtag could certainly include text that constitutes as hate speech. WIth this, and other hashtags that appear in the “Not Hate Speech” cloud, we can clearly see the very slight differences between the two labels.\n",
    "\n",
    "Besides that, others are simply pop culture references, such as #Scandal the TV show or #vote5sos referring to the boy band. It’s interesting that those contain a lot of offensive language, probably from fan reactions and community conflicts. Ultimately, we can recommend that **Twitter should closely monitor those top hashtags for potential posts containing hate speech** or even regular offensive language.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3. What is the overall polarity of the tweets?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![img3](./visualizations/compound_polarity_score.png)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The compound polarity score is a metric that calculates the sum of all the [lexicon ratings](https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt) which have been normalized between -1 and +1. With -1 being extreme negative and +1 being extreme positive. **This score encompasses the overall sentiment of this corpus.**\n",
    "\n",
    "- Hate Speech tweets on average have a compound score of -0.363\n",
    "- Non Hate Speech tweets on average have a compound score of -0.263\n",
    "\n",
    "According to this metric, both classes of tweets have pretty negative sentiments because their normalized compound scores are less than -0.05. \n",
    "\n",
    "Additionally from this graph, we can see that tweets classified as Hate Speech are especially negative. This further emphasizes how slim the difference between the two labels are. Although both classes contain negative and offensive language, Hate Speech is much more negative on average.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![img3](./visualizations/avg_polarity_by_tweet_type.png)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "To reiterate, this graph shows the average polarity scores for each label. We can see that a majority were scored as neutral. However, of those that were scored as negative, it seems like \"Not Hate Speech\" had more on average."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Class Imbalance\n",
    "\n",
    "The main roadblock of this dataset is the extreme class imbalance. We can see that only 5.77% of the data is labeled as hate speech. This could present challenges during the modeling process."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![imbalance](./visualizations/cleaned_class_imbalance.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "import pickle\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from sklearn.feature_extraction import text \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# modeling libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics, utils, model_selection, svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, plot_confusion_matrix, roc_curve, auc, classification_report"
   ]
  },
  {
   "source": [
    "# Preprocessing Text Data\n",
    "\n",
    "The original data from `twitter_data.csv` was cleaned using RegEx in the `data_cleaning.ipynb` notebook. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in clean_df\n",
    "clean_df = pd.read_pickle('./pickle/clean_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   total_votes  hate_speech_votes  other_votes  label  \\\n",
       "0            3                  0            3      0   \n",
       "1            3                  0            3      0   \n",
       "2            3                  0            3      0   \n",
       "\n",
       "                                               tweet  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...   \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...   \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...   \n",
       "\n",
       "                                        clean_tweets  \n",
       "0     as a woman you shouldnt complain about clea...  \n",
       "1     boy dats coldtyga dwn bad for cuffin dat ho...  \n",
       "2     dawg   you ever fuck a bitch and she sta to...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>total_votes</th>\n      <th>hate_speech_votes</th>\n      <th>other_votes</th>\n      <th>label</th>\n      <th>tweet</th>\n      <th>clean_tweets</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n      <td>as a woman you shouldnt complain about clea...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n      <td>boy dats coldtyga dwn bad for cuffin dat ho...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n      <td>dawg   you ever fuck a bitch and she sta to...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 194
    }
   ],
   "source": [
    "clean_df.head(3)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Tokenizing & Removing Stop Words\n",
    "\n",
    "When working with text data, one of the first steps is to remove stop words from the corpus. Although text would be gramatically incorrect without these stop words, they provide little value to models and typically hinder performace.\n",
    "\n",
    "We can use NLTK's built-in library of stop words to remove them in a tokenizing function.\n",
    "\n",
    "Additionally, we tokenize text data so that machine learning algorithms can understand it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# assigning variables to essential columns\n",
    "data = clean_df['clean_tweets']\n",
    "target = clean_df['label']"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 195,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting NLTK stop words as `stop_words`\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to tokeenize text in each column and remove stop words\n",
    "def process_tweet(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stopwords_removed = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
    "    return stopwords_removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the above function to our data/features \n",
    "processed_data = list(map(process_tweet, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20277"
      ]
     },
     "metadata": {},
     "execution_count": 199
    }
   ],
   "source": [
    "# getting count of all unique words in the corpus\n",
    "total_vocab = set()\n",
    "for comment in processed_data:\n",
    "    total_vocab.update(comment)\n",
    "len(total_vocab)"
   ]
  },
  {
   "source": [
    "We can see that the corpus has a vocabulary of 20,277 unique words."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Top Words in Corpus"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('bitch', 8227),\n",
       " ('bitches', 3083),\n",
       " ('like', 2766),\n",
       " ('hoes', 2368),\n",
       " ('pussy', 2099),\n",
       " ('im', 2061),\n",
       " ('hoe', 1906),\n",
       " ('dont', 1749),\n",
       " ('got', 1597),\n",
       " ('ass', 1570),\n",
       " ('get', 1428),\n",
       " ('fuck', 1411),\n",
       " ('u', 1280),\n",
       " ('shit', 1262),\n",
       " ('nigga', 1198),\n",
       " ('aint', 1158),\n",
       " ('trash', 1142),\n",
       " ('lol', 1074),\n",
       " ('know', 806),\n",
       " ('niggas', 791)]"
      ]
     },
     "metadata": {},
     "execution_count": 200
    }
   ],
   "source": [
    "# morphing `processed_data` into a readable list\n",
    "flat_filtered = [item for sublist in processed_data for item in sublist]\n",
    "# getting frequency distribution\n",
    "clean_corpus_freqdist = FreqDist(flat_filtered)\n",
    "# top 20 words in cleaned corpus\n",
    "clean_corpus_freqdist.most_common(20)"
   ]
  },
  {
   "source": [
    "These top words are mostly offensive terms, and some other slang words that the NLTK stop words removal function didn't pick up."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Lemmatization\n",
    "\n",
    "This last method reduces each word into a linguistically valid **lemma**, or root word. It does this through linguistic mappings, using the WordNet lexical database.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list with all lemmatized outputs\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "lemmatized_output = []\n",
    "\n",
    "for listy in processed_data:\n",
    "    lemmed = ' '.join([lemmatizer.lemmatize(w) for w in listy])\n",
    "    lemmatized_output.append(lemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lem = lemmatized_output\n",
    "y_lem = target"
   ]
  },
  {
   "source": [
    "Now `X_lem` and `y_lem` are contain cleaned text and the original labels, ready to be used for modeling."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Feature Engineering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Train-Test Split"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_lem, y_lem, test_size=0.20, random_state=15)"
   ]
  },
  {
   "source": [
    "## TF-IDF Vectorization\n",
    "\n",
    "Before tokenized text data can be fed into machine learning models, they must be transformed into numerical feature vectors. First, we’ll be trying one of the most popular methods, TF-IDF Vectorization.\n",
    "\n",
    "This is an acronym than stands for “Term Frequency — Inverse Document” Frequency which are the components of the resulting scores assigned to each word.\n",
    "- Term Frequency: This summarizes how often a given word appears within a document.\n",
    "- Inverse Document Frequency: This down scales words that appear a lot across documents.\n",
    "\n",
    "Without going into the math, TF-IDF are word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but not across other documents."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using tf_idf vectorizor\n",
    "tfidf = TfidfVectorizer(stop_words= stop_words, ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse matrix format with 265K stored elements\n",
    "tfidf_data_train = tfidf.fit_transform(X_train)\n",
    "tfidf_data_test = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Average Number of Non-Zero Elements in Vectorized Articles: 13.364420457984465\nPercentage of columns containing ZERO: 0.9998720483637183\n"
     ]
    }
   ],
   "source": [
    "# taking a quick look of the non zero elements\n",
    "non_zero_cols = tfidf_data_train.nnz / float(tfidf_data_train.shape[0])\n",
    "print(\"Average Number of Non-Zero Elements in Vectorized Articles: {}\".format(non_zero_cols))\n",
    "\n",
    "percent_sparse = 1 - (non_zero_cols / float(tfidf_data_train.shape[1]))\n",
    "print('Percentage of columns containing ZERO: {}'.format(percent_sparse))"
   ]
  },
  {
   "source": [
    "99.9% of the columns contain a zero, meaning that's a very **sparse matrix** with 265K columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Baseline Modeling\n",
    "\n",
    "We use the evaluation metrics Precision, Recall and F1 score to determine success. However, F1 is valued as the “most important” metric because by finding an equal balance between Precision and Recall, it's useful for data with high class imbalance.\n",
    "\n",
    "Overall, we want as much hate speech to flagged as possible and so that it can be efficiently removed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to print evaluation metrics\n",
    "def evaluation(precision, recall, f1, f1_weighted):\n",
    "    \"\"\"prints out evaluation metrics for a model\"\"\"\n",
    "    print('Testing Evaluation Metrics:')\n",
    "    print('Precision: {:.4}'.format(precision))\n",
    "    print('Recall: {:.4}'.format(recall))\n",
    "    print('F1 Score: {:.4}'.format(f1))\n",
    "    print('Weighted F1 Score: {:.4}'.format(f1_weighted))"
   ]
  },
  {
   "source": [
    "## Baseline Random Forest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_baseline = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=0)\n",
    "# adding class_weight='balanced' increased accuracy & precision but decreased F1"
   ]
  },
  {
   "source": [
    "⏳ the cell below takes about 38 seconds to run"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 37.1 s, sys: 142 ms, total: 37.3 s\nWall time: 37.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf_baseline.fit(tfidf_data_train, y_train)\n",
    "rf_test_preds = rf_baseline.predict(tfidf_data_test)"
   ]
  },
  {
   "source": [
    "rf_precision = precision_score(y_test, rf_test_preds)\n",
    "rf_recall = recall_score(y_test, rf_test_preds)\n",
    "rf_f1_score = f1_score(y_test, rf_test_preds)\n",
    "rf_f1_weighted = f1_score(y_test, rf_test_preds, average='weighted')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 210,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Evaluation Metrics:\nPrecision: 0.4128\nRecall: 0.1613\nF1 Score: 0.232\nWeighted F1 Score: 0.9272\n"
     ]
    }
   ],
   "source": [
    "evaluation(rf_precision, rf_recall, rf_f1_score, rf_f1_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dictionary to store all metrics\n",
    "metric_dict = {}\n",
    "# adding scores to metric_dict\n",
    "metric_dict['Baseline Random Forest'] = {'precision': rf_precision, 'recall': rf_recall, 'f1_score': rf_f1_score, 'weighted_f1': rf_f1_weighted}"
   ]
  },
  {
   "source": [
    "## Baseline Logistic Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_baseline = LogisticRegression(penalty='l2', class_weight='balanced', random_state=20)\n",
    "# class_weight='balanced' actually didn't impact the score"
   ]
  },
  {
   "source": [
    "⏳ the cell below takes about 3 seconds to run"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 3.15 s, sys: 229 ms, total: 3.38 s\nWall time: 1.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logreg_baseline.fit(tfidf_data_train, y_train)\n",
    "logreg_test_preds = logreg_baseline.predict(tfidf_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_precision = precision_score(y_test, logreg_test_preds)\n",
    "logreg_recall = recall_score(y_test, logreg_test_preds)\n",
    "logreg_f1_score = f1_score(y_test, logreg_test_preds)\n",
    "logreg_f1_weighted = f1_score(y_test, logreg_test_preds, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Evaluation Metrics:\nPrecision: 0.2939\nRecall: 0.5699\nF1 Score: 0.3878\nWeighted F1 Score: 0.9134\n"
     ]
    }
   ],
   "source": [
    "evaluation(logreg_precision, logreg_recall, logreg_f1_score, logreg_f1_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding scores to metric_dict\n",
    "metric_dict['Baseline Logisitic Regression'] = {'precision': logreg_precision, 'recall': logreg_recall, 'f1_score': logreg_f1_score, 'weighted_f1': logreg_f1_weighted}"
   ]
  },
  {
   "source": [
    "## Baseline Naive Bayes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_bayes = MultinomialNB(alpha = .01)"
   ]
  },
  {
   "source": [
    "⏳ the cell below takes about 11 seconds to run"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 10.6 ms, sys: 5.97 ms, total: 16.6 ms\nWall time: 15.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "baseline_bayes.fit(tfidf_data_train, y_train)\n",
    "bayes_test_preds = baseline_bayes.predict(tfidf_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_precision = precision_score(y_test, bayes_test_preds)\n",
    "bayes_recall = recall_score(y_test, bayes_test_preds)\n",
    "bayes_f1_score = f1_score(y_test, bayes_test_preds)\n",
    "bayes_f1_weighted = f1_score(y_test, bayes_test_preds, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Evaluation Metrics:\nPrecision: 0.4118\nRecall: 0.1254\nF1 Score: 0.1923\nWeighted F1 Score: 0.9255\n"
     ]
    }
   ],
   "source": [
    "evaluation(bayes_precision, bayes_recall, bayes_f1_score, bayes_f1_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding scores to metric_dict\n",
    "metric_dict['Baseline Naive Bayes'] = {'precision': bayes_precision, 'recall': bayes_recall, 'f1_score': bayes_f1_score, 'weighted_f1': bayes_f1_weighted}"
   ]
  },
  {
   "source": [
    "## Baseline Support Vector Machine (SVM)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_baseline = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto', class_weight='balanced')"
   ]
  },
  {
   "source": [
    "⏳ the cell below takes about 1 minute to run"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 48.6 s, sys: 663 ms, total: 49.2 s\nWall time: 49.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# fit the training dataset on the classifier\n",
    "SVM_baseline.fit(tfidf_data_train, y_train)\n",
    "# predict the labels on validation dataset\n",
    "SVM_test_preds = SVM_baseline.predict(tfidf_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_precision = precision_score(y_test, SVM_test_preds)\n",
    "SVM_recall = recall_score(y_test, SVM_test_preds)\n",
    "SVM_f1_score = f1_score(y_test, SVM_test_preds)\n",
    "SVM_f1_weighted = f1_score(y_test, SVM_test_preds, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Evaluation Metrics:\nPrecision: 0.3609\nRecall: 0.4373\nF1 Score: 0.3955\nWeighted F1 Score: 0.9281\n"
     ]
    }
   ],
   "source": [
    "evaluation(SVM_precision, SVM_recall, SVM_f1_score, SVM_f1_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding scores to metric_dict\n",
    "metric_dict['Baseline SVM'] = {'precision': SVM_precision, 'recall': SVM_recall, 'f1_score': SVM_f1_score, 'weighted_f1': SVM_f1_weighted}"
   ]
  },
  {
   "source": [
    "## Evaluation Metrics for All Baseline Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                               precision    recall  f1_score  weighted_f1\n",
       "Baseline Random Forest          0.412844  0.161290  0.231959     0.927249\n",
       "Baseline Logisitic Regression   0.293900  0.569892  0.387805     0.913449\n",
       "Baseline Naive Bayes            0.411765  0.125448  0.192308     0.925487\n",
       "Baseline SVM                    0.360947  0.437276  0.395462     0.928112"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>f1_score</th>\n      <th>weighted_f1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Baseline Random Forest</th>\n      <td>0.412844</td>\n      <td>0.161290</td>\n      <td>0.231959</td>\n      <td>0.927249</td>\n    </tr>\n    <tr>\n      <th>Baseline Logisitic Regression</th>\n      <td>0.293900</td>\n      <td>0.569892</td>\n      <td>0.387805</td>\n      <td>0.913449</td>\n    </tr>\n    <tr>\n      <th>Baseline Naive Bayes</th>\n      <td>0.411765</td>\n      <td>0.125448</td>\n      <td>0.192308</td>\n      <td>0.925487</td>\n    </tr>\n    <tr>\n      <th>Baseline SVM</th>\n      <td>0.360947</td>\n      <td>0.437276</td>\n      <td>0.395462</td>\n      <td>0.928112</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 228
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(metric_dict, orient='index')"
   ]
  },
  {
   "source": [
    "Overall, the SVM model performed the best across unweighted and weighted F1. However, there is still work to be done to bring that score up higher. Next we'll try improving that model with grid search."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Doc2Vec\n",
    "\n",
    "One disadvantage to using Count Vectorization or TF-DF Vectorization is that we can run into the Curse of Dimensionality. This is because these methods create Sparse Vectors, that are the length of the total vocabulary of the text corpus. Therefore, this extra space of 99%s could possibly hurt the model. \n",
    "\n",
    "We can try Doc2Vec, which is an extension of Word2Vec. It aims to learn how to project a document into a latent d-dimensional space. Specifically, the Distributed Bag of Words (DBOW) model ignores the context words in the input, but instead forces the model to predict words randomly sampled from the paragraph in the output."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fresh TTS for doc2vec data\n",
    "doc_train, doc_test = train_test_split(clean_df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "source": [
    "## Data Preparation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_train = doc_train.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['clean_tweets']), tags=[r.label]), axis=1)\n",
    "tagged_test = doc_test.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['clean_tweets']), tags=[r.label]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TaggedDocument(words=['this', 'bitch', 'instating', 'and', 'driving', 'for', 'me'], tags=[0])"
      ]
     },
     "metadata": {},
     "execution_count": 232
    }
   ],
   "source": [
    "tagged_train.values[30]"
   ]
  },
  {
   "source": [
    "## Training DBOW Model\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a doc2vec model, using only training data\n",
    "dbow_model = Doc2Vec(vector_size=100, \n",
    "                alpha=0.025, \n",
    "                min_count=5,\n",
    "                dm=1, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 17348/17348 [00:00<00:00, 2979273.05it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "# building vocabulary \n",
    "dbow_model.build_vocab([x for x in tqdm(tagged_train.values)])\n"
   ]
  },
  {
   "source": [
    "⏳ the cell below takes about 26 seconds to run"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 17348/17348 [00:00<00:00, 2920558.15it/s]\n",
      "100%|██████████| 17348/17348 [00:00<00:00, 3042812.94it/s]\n",
      "100%|██████████| 17348/17348 [00:00<00:00, 3131872.16it/s]\n",
      "100%|██████████| 17348/17348 [00:00<00:00, 3488483.35it/s]\n",
      "100%|██████████| 17348/17348 [00:00<00:00, 3441459.86it/s]\n",
      "100%|██████████| 17348/17348 [00:00<00:00, 3215466.25it/s]\n",
      "100%|██████████| 17348/17348 [00:00<00:00, 3137679.42it/s]\n",
      "100%|██████████| 17348/17348 [00:00<00:00, 2875885.77it/s]\n",
      "100%|██████████| 17348/17348 [00:00<00:00, 3223871.77it/s]\n",
      "100%|██████████| 17348/17348 [00:00<00:00, 2119201.57it/s]\n",
      "100%|██████████| 17348/17348 [00:00<00:00, 3008218.36it/s]\n",
      "100%|██████████| 17348/17348 [00:00<00:00, 3198364.21it/s]\n",
      "100%|██████████| 17348/17348 [00:00<00:00, 3191490.23it/s]\n",
      "100%|██████████| 17348/17348 [00:00<00:00, 3080819.11it/s]\n",
      "100%|██████████| 17348/17348 [00:00<00:00, 3033299.39it/s]\n",
      "100%|██████████| 17348/17348 [00:00<00:00, 3173672.34it/s]\n",
      "100%|██████████| 17348/17348 [00:00<00:00, 2896261.82it/s]\n",
      "100%|██████████| 17348/17348 [00:00<00:00, 3311915.60it/s]\n",
      "100%|██████████| 17348/17348 [00:00<00:00, 3293775.10it/s]\n",
      "100%|██████████| 17348/17348 [00:00<00:00, 3237786.94it/s]\n",
      "100%|██████████| 17348/17348 [00:00<00:00, 3088927.91it/s]\n",
      "100%|██████████| 17348/17348 [00:00<00:00, 3223157.73it/s]\n",
      "100%|██████████| 17348/17348 [00:00<00:00, 2728468.04it/s]\n",
      "100%|██████████| 17348/17348 [00:00<00:00, 2970152.09it/s]\n",
      "100%|██████████| 17348/17348 [00:00<00:00, 3137814.73it/s]\n",
      "100%|██████████| 17348/17348 [00:00<00:00, 3156463.03it/s]\n",
      "100%|██████████| 17348/17348 [00:00<00:00, 2830135.58it/s]\n",
      "100%|██████████| 17348/17348 [00:00<00:00, 3131333.04it/s]\n",
      "100%|██████████| 17348/17348 [00:00<00:00, 3024347.89it/s]\n",
      "100%|██████████| 17348/17348 [00:00<00:00, 2692923.23it/s]\n",
      "CPU times: user 27.7 s, sys: 4.39 s, total: 32.1 s\n",
      "Wall time: 24.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    dbow_model.train(utils.shuffle([x for x in tqdm(tagged_train.values)]), total_examples=len(tagged_train.values), epochs=1)\n",
    "    dbow_model.alpha -= 0.002\n",
    "    dbow_model.min_alpha = dbow_model.alpha"
   ]
  },
  {
   "source": [
    "### Final Vector Feature for Classification Model Use"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "source": [
    "## Applying Doc2Vec Data to SVM Baseline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "doc_y_train, doc_X_train = vec_for_learning(dbow_model, tagged_train)\n",
    "doc_y_test, doc_X_test = vec_for_learning(dbow_model, tagged_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc2Vec_SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto', class_weight='balanced')"
   ]
  },
  {
   "source": [
    "⏳ the cell below takes about 28 seconds to run"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 27.8 s, sys: 282 ms, total: 28.1 s\nWall time: 28.1 s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SVC(class_weight='balanced', gamma='auto', kernel='linear')"
      ]
     },
     "metadata": {},
     "execution_count": 239
    }
   ],
   "source": [
    "%%time\n",
    "# fit the training dataset on the classifier\n",
    "Doc2Vec_SVM.fit(doc_X_train, doc_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the labels on validation dataset\n",
    "doc_SVM_y_preds = Doc2Vec_SVM.predict(doc_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_SVM_precision = precision_score(doc_y_test, doc_SVM_y_preds)\n",
    "doc_SVM_recall = recall_score(doc_y_test, doc_SVM_y_preds)\n",
    "doc_SVM_f1_score = f1_score(doc_y_test, doc_SVM_y_preds)\n",
    "doc_SVM_f1_weighted = f1_score(doc_y_test, doc_SVM_y_preds, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Evaluation Metrics:\nPrecision: 0.1879\nRecall: 0.6253\nF1 Score: 0.289\nWeighted F1 Score: 0.8641\n"
     ]
    }
   ],
   "source": [
    "evaluation(doc_SVM_precision, doc_SVM_recall, doc_SVM_f1_score, doc_SVM_f1_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dict['Baseline SVM with Doc2Vec'] = {'precision': doc_SVM_precision, 'recall': doc_SVM_recall, 'f1_score': doc_SVM_f1_score, 'weighted_f1': doc_SVM_f1_weighted}"
   ]
  },
  {
   "source": [
    "## Comparing with Other Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                               precision    recall  f1_score  weighted_f1\n",
       "Baseline Random Forest          0.412844  0.161290  0.231959     0.927249\n",
       "Baseline Logisitic Regression   0.293900  0.569892  0.387805     0.913449\n",
       "Baseline Naive Bayes            0.411765  0.125448  0.192308     0.925487\n",
       "Baseline SVM                    0.360947  0.437276  0.395462     0.928112\n",
       "Baseline SVM with Doc2Vec       0.187896  0.625293  0.288961     0.864053"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>f1_score</th>\n      <th>weighted_f1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Baseline Random Forest</th>\n      <td>0.412844</td>\n      <td>0.161290</td>\n      <td>0.231959</td>\n      <td>0.927249</td>\n    </tr>\n    <tr>\n      <th>Baseline Logisitic Regression</th>\n      <td>0.293900</td>\n      <td>0.569892</td>\n      <td>0.387805</td>\n      <td>0.913449</td>\n    </tr>\n    <tr>\n      <th>Baseline Naive Bayes</th>\n      <td>0.411765</td>\n      <td>0.125448</td>\n      <td>0.192308</td>\n      <td>0.925487</td>\n    </tr>\n    <tr>\n      <th>Baseline SVM</th>\n      <td>0.360947</td>\n      <td>0.437276</td>\n      <td>0.395462</td>\n      <td>0.928112</td>\n    </tr>\n    <tr>\n      <th>Baseline SVM with Doc2Vec</th>\n      <td>0.187896</td>\n      <td>0.625293</td>\n      <td>0.288961</td>\n      <td>0.864053</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 244
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(metric_dict, orient='index')"
   ]
  },
  {
   "source": [
    "Looks like the Doc2Vec model didn't perform as well. Let's look at this closer with a classification report, so see how both models predicted each label."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## SVM Baselines: TF-IDF vs. Doc2Vec"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------- SVM Baseline with TF-IDF -------------\n              precision    recall  f1-score   support\n\n     class 0       0.97      0.95      0.96      4678\n     class 1       0.36      0.44      0.40       279\n\n    accuracy                           0.92      4957\n   macro avg       0.66      0.70      0.68      4957\nweighted avg       0.93      0.92      0.93      4957\n\n------------- SVM Baseline with Doc2Vec -------------\n              precision    recall  f1-score   support\n\n     class 0       0.97      0.84      0.90      7008\n     class 1       0.19      0.63      0.29       427\n\n    accuracy                           0.82      7435\n   macro avg       0.58      0.73      0.59      7435\nweighted avg       0.93      0.82      0.86      7435\n\n"
     ]
    }
   ],
   "source": [
    "target_names = ['class 0', 'class 1']\n",
    "# Tf-IDF baseline\n",
    "print('-'*14 + ' SVM Baseline with TF-IDF ' + '-'*13)\n",
    "print(classification_report(y_test, SVM_test_preds, target_names=target_names))\n",
    "# Doc2Vec baseline\n",
    "print('-'*13 + ' SVM Baseline with Doc2Vec ' + '-'*13)\n",
    "print(classification_report(doc_y_test, doc_SVM_y_preds, target_names=target_names))"
   ]
  },
  {
   "source": [
    "The Doc2Vec method actually performed worse than the TF-IDF method. We can see that it didnt predict the 1 or 0 label as well as the TF-IDF Vectorized model.\n",
    "\n",
    "Let's go back to the original SVM Baseline and try other class imbalance remedy methods to improve the scores"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Dealing with Class Imbalance\n",
    "\n",
    "The initial SVM Baseline dealt with class imbalance with the parameter `class_weight='balanced'`. Let's try other clas imbalance remedy methods."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Over-Sampling with SMOTE\n",
    "Over-samples the minority class, hate speech."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=35)\n",
    "smote_X_train, smote_y_train = sm.fit_sample(tfidf_data_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto', random_state=15)"
   ]
  },
  {
   "source": [
    "⏳ the cell below takes about 4 minutes to run"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 3min 52s, sys: 2.45 s, total: 3min 54s\nWall time: 3min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "smote_SVM.fit(smote_X_train, smote_y_train)\n",
    "smote_SVM_test_preds = smote_SVM.predict(tfidf_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_precision = precision_score(y_test, smote_SVM_test_preds)\n",
    "smote_recall = recall_score(y_test, smote_SVM_test_preds)\n",
    "smote_f1_score = f1_score(y_test, smote_SVM_test_preds)\n",
    "smote_weighted_f1_score = f1_score(y_test, smote_SVM_test_preds, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Evaluation Metrics:\nPrecision: 0.3393\nRecall: 0.2724\nF1 Score: 0.3022\nWeighted F1 Score: 0.9255\n"
     ]
    }
   ],
   "source": [
    "evaluation(smote_precision, smote_recall, smote_f1_score, smote_weighted_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding these metrics to evaluation metric dict\n",
    "metric_dict['SVM Oversampled with SMOTE'] = {'precision': smote_precision, 'recall': smote_recall, 'f1_score': smote_f1_score, 'weighted_f1': smote_weighted_f1_score}"
   ]
  },
  {
   "source": [
    "## Under-Sampling with TomeK Links\n",
    "Under-samples the majoirty class, not hate speech."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Resampled dataset shape Counter({0: 18627, 1: 1151})\n"
     ]
    }
   ],
   "source": [
    "tl = TomekLinks()\n",
    "tomek_X_train, tomek_y_train = tl.fit_resample(tfidf_data_train, y_train)\n",
    "print('Resampled dataset shape %s' % Counter(tomek_y_train))"
   ]
  },
  {
   "source": [
    "It only removed 48 values from the majority class."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomek_SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto', random_state=15)"
   ]
  },
  {
   "source": [
    "⏳ the cell below takes about 43 seconds to run"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 42.3 s, sys: 782 ms, total: 43.1 s\nWall time: 43.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tomek_SVM.fit(tomek_X_train, tomek_y_train)\n",
    "tomek_logreg_test_preds = tomek_SVM.predict(tfidf_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomek_precision = precision_score(y_test, tomek_logreg_test_preds)\n",
    "tomek_recall = recall_score(y_test, tomek_logreg_test_preds)\n",
    "tomek_f1_score = f1_score(y_test, tomek_logreg_test_preds)\n",
    "tomek_weighted_f1_score = f1_score(y_test, tomek_logreg_test_preds, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Evaluation Metrics:\nPrecision: 0.6562\nRecall: 0.2258\nF1 Score: 0.336\nWeighted F1 Score: 0.938\n"
     ]
    }
   ],
   "source": [
    "evaluation(tomek_precision, tomek_recall, tomek_f1_score, tomek_weighted_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding these metrics to evaluation metric dict\n",
    "metric_dict['SVM Undersampled with Tomek Links'] = {'precision': tomek_precision, 'recall': tomek_recall, 'f1_score': tomek_f1_score, 'weighted_f1': tomek_weighted_f1_score}"
   ]
  },
  {
   "source": [
    "## Comparing with Other Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                   precision    recall  f1_score  weighted_f1\n",
       "Baseline Random Forest              0.412844  0.161290  0.231959     0.927249\n",
       "Baseline Logisitic Regression       0.293900  0.569892  0.387805     0.913449\n",
       "Baseline Naive Bayes                0.411765  0.125448  0.192308     0.925487\n",
       "Baseline SVM                        0.360947  0.437276  0.395462     0.928112\n",
       "Baseline SVM with Doc2Vec           0.187896  0.625293  0.288961     0.864053\n",
       "SVM Oversampled with SMOTE          0.339286  0.272401  0.302187     0.925527\n",
       "SVM Undersampled with Tomek Links   0.656250  0.225806  0.336000     0.937993"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>f1_score</th>\n      <th>weighted_f1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Baseline Random Forest</th>\n      <td>0.412844</td>\n      <td>0.161290</td>\n      <td>0.231959</td>\n      <td>0.927249</td>\n    </tr>\n    <tr>\n      <th>Baseline Logisitic Regression</th>\n      <td>0.293900</td>\n      <td>0.569892</td>\n      <td>0.387805</td>\n      <td>0.913449</td>\n    </tr>\n    <tr>\n      <th>Baseline Naive Bayes</th>\n      <td>0.411765</td>\n      <td>0.125448</td>\n      <td>0.192308</td>\n      <td>0.925487</td>\n    </tr>\n    <tr>\n      <th>Baseline SVM</th>\n      <td>0.360947</td>\n      <td>0.437276</td>\n      <td>0.395462</td>\n      <td>0.928112</td>\n    </tr>\n    <tr>\n      <th>Baseline SVM with Doc2Vec</th>\n      <td>0.187896</td>\n      <td>0.625293</td>\n      <td>0.288961</td>\n      <td>0.864053</td>\n    </tr>\n    <tr>\n      <th>SVM Oversampled with SMOTE</th>\n      <td>0.339286</td>\n      <td>0.272401</td>\n      <td>0.302187</td>\n      <td>0.925527</td>\n    </tr>\n    <tr>\n      <th>SVM Undersampled with Tomek Links</th>\n      <td>0.656250</td>\n      <td>0.225806</td>\n      <td>0.336000</td>\n      <td>0.937993</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 260
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(metric_dict, orient='index')"
   ]
  },
  {
   "source": [
    "- The baseline SVM with `class_weight=balanced` has the highest unweighted F1\n",
    "- The undersampled baseline has a lower raw F1, but higher weighted F1.\n",
    "\n",
    "We can take a look at each model's classification report to get a better idea about what's happening."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--------- SVM with `class_weight=balanced` ----------\n              precision    recall  f1-score   support\n\n     class 0       0.97      0.95      0.96      4678\n     class 1       0.36      0.44      0.40       279\n\n    accuracy                           0.92      4957\n   macro avg       0.66      0.70      0.68      4957\nweighted avg       0.93      0.92      0.93      4957\n\n--------- SVM Undersampled with Tomek Links ---------\n              precision    recall  f1-score   support\n\n     class 0       0.96      0.99      0.97      4678\n     class 1       0.66      0.23      0.34       279\n\n    accuracy                           0.95      4957\n   macro avg       0.81      0.61      0.65      4957\nweighted avg       0.94      0.95      0.94      4957\n\n"
     ]
    }
   ],
   "source": [
    "target_names = ['class 0', 'class 1']\n",
    "# class_weight='balanced' Baseline report\n",
    "print('-'*9 + ' SVM with `class_weight=balanced` ' + '-'*10)\n",
    "print(classification_report(y_test, SVM_test_preds, target_names=target_names))\n",
    "print('-'*9 + ' SVM Undersampled with Tomek Links ' + '-'*9)\n",
    "# Undersampled Baseline report\n",
    "print(classification_report(y_test, tomek_logreg_test_preds, target_names=target_names))"
   ]
  },
  {
   "source": [
    "There are some differances. But most noteably, the baseline with `class_weight=balanced` predicts the hate speech (1) class much better than the other model. \n",
    "\n",
    "Therefore, let's stick with that one and grid search to tune its hyperparameters."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Grid Search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the model\n",
    "grid_model = svm.SVC(degree=3, class_weight='balanced', random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating param_dict\n",
    "param_dict={'C': [1, 10, 100],  \n",
    "              'gamma': [0.1, 0.01, 0.001], \n",
    "              'kernel': ['rbf', 'sigmoid']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate Grid Search CV with F1 metric\n",
    "grid_baseline = GridSearchCV(grid_model, param_dict, cv=5, scoring='f1', verbose=3)"
   ]
  },
  {
   "source": [
    "⏳ the cell below takes about **49 minutes** to run"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[CV] .......... C=1, gamma=0.1, kernel=rbf, score=0.433, total=  24.6s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   24.6s remaining:    0.0s\n",
      "[CV] .......... C=1, gamma=0.1, kernel=rbf, score=0.465, total=  25.8s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   50.4s remaining:    0.0s\n",
      "[CV] .......... C=1, gamma=0.1, kernel=rbf, score=0.446, total=  25.6s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] .......... C=1, gamma=0.1, kernel=rbf, score=0.456, total=  25.3s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] .......... C=1, gamma=0.1, kernel=rbf, score=0.405, total=  24.7s\n",
      "[CV] C=1, gamma=0.1, kernel=sigmoid ..................................\n",
      "[CV] ...... C=1, gamma=0.1, kernel=sigmoid, score=0.439, total=  28.5s\n",
      "[CV] C=1, gamma=0.1, kernel=sigmoid ..................................\n",
      "[CV] ...... C=1, gamma=0.1, kernel=sigmoid, score=0.451, total=  28.2s\n",
      "[CV] C=1, gamma=0.1, kernel=sigmoid ..................................\n",
      "[CV] ...... C=1, gamma=0.1, kernel=sigmoid, score=0.441, total=  28.6s\n",
      "[CV] C=1, gamma=0.1, kernel=sigmoid ..................................\n",
      "[CV] ...... C=1, gamma=0.1, kernel=sigmoid, score=0.450, total=  28.1s\n",
      "[CV] C=1, gamma=0.1, kernel=sigmoid ..................................\n",
      "[CV] ...... C=1, gamma=0.1, kernel=sigmoid, score=0.400, total=  29.0s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ......... C=1, gamma=0.01, kernel=rbf, score=0.360, total=  34.1s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ......... C=1, gamma=0.01, kernel=rbf, score=0.416, total=  34.2s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ......... C=1, gamma=0.01, kernel=rbf, score=0.390, total=  33.8s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ......... C=1, gamma=0.01, kernel=rbf, score=0.376, total=  33.4s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ......... C=1, gamma=0.01, kernel=rbf, score=0.367, total=  34.3s\n",
      "[CV] C=1, gamma=0.01, kernel=sigmoid .................................\n",
      "[CV] ..... C=1, gamma=0.01, kernel=sigmoid, score=0.165, total=  34.8s\n",
      "[CV] C=1, gamma=0.01, kernel=sigmoid .................................\n",
      "[CV] ..... C=1, gamma=0.01, kernel=sigmoid, score=0.164, total=  35.7s\n",
      "[CV] C=1, gamma=0.01, kernel=sigmoid .................................\n",
      "[CV] ..... C=1, gamma=0.01, kernel=sigmoid, score=0.157, total=  35.0s\n",
      "[CV] C=1, gamma=0.01, kernel=sigmoid .................................\n",
      "[CV] ..... C=1, gamma=0.01, kernel=sigmoid, score=0.109, total=  35.2s\n",
      "[CV] C=1, gamma=0.01, kernel=sigmoid .................................\n",
      "[CV] ..... C=1, gamma=0.01, kernel=sigmoid, score=0.185, total=  35.3s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV] ........ C=1, gamma=0.001, kernel=rbf, score=0.034, total=  35.5s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV] ........ C=1, gamma=0.001, kernel=rbf, score=0.110, total=  35.4s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV] ........ C=1, gamma=0.001, kernel=rbf, score=0.110, total=  35.6s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV] ........ C=1, gamma=0.001, kernel=rbf, score=0.110, total=  34.8s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV] ........ C=1, gamma=0.001, kernel=rbf, score=0.110, total=  35.0s\n",
      "[CV] C=1, gamma=0.001, kernel=sigmoid ................................\n",
      "[CV] .... C=1, gamma=0.001, kernel=sigmoid, score=0.000, total=  37.4s\n",
      "[CV] C=1, gamma=0.001, kernel=sigmoid ................................\n",
      "[CV] .... C=1, gamma=0.001, kernel=sigmoid, score=0.110, total=  35.8s\n",
      "[CV] C=1, gamma=0.001, kernel=sigmoid ................................\n",
      "[CV] .... C=1, gamma=0.001, kernel=sigmoid, score=0.110, total=  35.9s\n",
      "[CV] C=1, gamma=0.001, kernel=sigmoid ................................\n",
      "[CV] .... C=1, gamma=0.001, kernel=sigmoid, score=0.110, total=  35.4s\n",
      "[CV] C=1, gamma=0.001, kernel=sigmoid ................................\n",
      "[CV] .... C=1, gamma=0.001, kernel=sigmoid, score=0.110, total=  35.8s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV] ......... C=10, gamma=0.1, kernel=rbf, score=0.312, total=  37.4s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV] ......... C=10, gamma=0.1, kernel=rbf, score=0.267, total=  39.3s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV] ......... C=10, gamma=0.1, kernel=rbf, score=0.253, total=  39.4s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV] ......... C=10, gamma=0.1, kernel=rbf, score=0.319, total=  38.6s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV] ......... C=10, gamma=0.1, kernel=rbf, score=0.282, total=  37.4s\n",
      "[CV] C=10, gamma=0.1, kernel=sigmoid .................................\n",
      "[CV] ..... C=10, gamma=0.1, kernel=sigmoid, score=0.358, total=  32.1s\n",
      "[CV] C=10, gamma=0.1, kernel=sigmoid .................................\n",
      "[CV] ..... C=10, gamma=0.1, kernel=sigmoid, score=0.383, total=  32.7s\n",
      "[CV] C=10, gamma=0.1, kernel=sigmoid .................................\n",
      "[CV] ..... C=10, gamma=0.1, kernel=sigmoid, score=0.331, total=  34.4s\n",
      "[CV] C=10, gamma=0.1, kernel=sigmoid .................................\n",
      "[CV] ..... C=10, gamma=0.1, kernel=sigmoid, score=0.409, total=  33.7s\n",
      "[CV] C=10, gamma=0.1, kernel=sigmoid .................................\n",
      "[CV] ..... C=10, gamma=0.1, kernel=sigmoid, score=0.373, total=  31.4s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV] ........ C=10, gamma=0.01, kernel=rbf, score=0.431, total=  24.2s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV] ........ C=10, gamma=0.01, kernel=rbf, score=0.459, total=  24.5s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV] ........ C=10, gamma=0.01, kernel=rbf, score=0.444, total=  24.4s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV] ........ C=10, gamma=0.01, kernel=rbf, score=0.458, total=  24.7s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV] ........ C=10, gamma=0.01, kernel=rbf, score=0.414, total=  24.8s\n",
      "[CV] C=10, gamma=0.01, kernel=sigmoid ................................\n",
      "[CV] .... C=10, gamma=0.01, kernel=sigmoid, score=0.439, total=  29.2s\n",
      "[CV] C=10, gamma=0.01, kernel=sigmoid ................................\n",
      "[CV] .... C=10, gamma=0.01, kernel=sigmoid, score=0.451, total=  29.4s\n",
      "[CV] C=10, gamma=0.01, kernel=sigmoid ................................\n",
      "[CV] .... C=10, gamma=0.01, kernel=sigmoid, score=0.441, total=  28.7s\n",
      "[CV] C=10, gamma=0.01, kernel=sigmoid ................................\n",
      "[CV] .... C=10, gamma=0.01, kernel=sigmoid, score=0.444, total=  28.9s\n",
      "[CV] C=10, gamma=0.01, kernel=sigmoid ................................\n",
      "[CV] .... C=10, gamma=0.01, kernel=sigmoid, score=0.400, total=  29.4s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] ....... C=10, gamma=0.001, kernel=rbf, score=0.359, total=  35.2s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] ....... C=10, gamma=0.001, kernel=rbf, score=0.419, total=  34.9s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] ....... C=10, gamma=0.001, kernel=rbf, score=0.392, total=  34.7s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] ....... C=10, gamma=0.001, kernel=rbf, score=0.383, total=  34.5s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] ....... C=10, gamma=0.001, kernel=rbf, score=0.367, total=  34.4s\n",
      "[CV] C=10, gamma=0.001, kernel=sigmoid ...............................\n",
      "[CV] ... C=10, gamma=0.001, kernel=sigmoid, score=0.165, total=  34.4s\n",
      "[CV] C=10, gamma=0.001, kernel=sigmoid ...............................\n",
      "[CV] ... C=10, gamma=0.001, kernel=sigmoid, score=0.164, total=  34.2s\n",
      "[CV] C=10, gamma=0.001, kernel=sigmoid ...............................\n",
      "[CV] ... C=10, gamma=0.001, kernel=sigmoid, score=0.157, total=  34.7s\n",
      "[CV] C=10, gamma=0.001, kernel=sigmoid ...............................\n",
      "[CV] ... C=10, gamma=0.001, kernel=sigmoid, score=0.109, total=  34.3s\n",
      "[CV] C=10, gamma=0.001, kernel=sigmoid ...............................\n",
      "[CV] ... C=10, gamma=0.001, kernel=sigmoid, score=0.185, total=  34.7s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ........ C=100, gamma=0.1, kernel=rbf, score=0.256, total=  37.5s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ........ C=100, gamma=0.1, kernel=rbf, score=0.240, total=  37.8s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ........ C=100, gamma=0.1, kernel=rbf, score=0.241, total=  39.9s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ........ C=100, gamma=0.1, kernel=rbf, score=0.279, total=  37.9s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ........ C=100, gamma=0.1, kernel=rbf, score=0.274, total=  36.5s\n",
      "[CV] C=100, gamma=0.1, kernel=sigmoid ................................\n",
      "[CV] .... C=100, gamma=0.1, kernel=sigmoid, score=0.257, total=  36.9s\n",
      "[CV] C=100, gamma=0.1, kernel=sigmoid ................................\n",
      "[CV] .... C=100, gamma=0.1, kernel=sigmoid, score=0.237, total=  39.1s\n",
      "[CV] C=100, gamma=0.1, kernel=sigmoid ................................\n",
      "[CV] .... C=100, gamma=0.1, kernel=sigmoid, score=0.244, total=  38.9s\n",
      "[CV] C=100, gamma=0.1, kernel=sigmoid ................................\n",
      "[CV] .... C=100, gamma=0.1, kernel=sigmoid, score=0.266, total=  35.4s\n",
      "[CV] C=100, gamma=0.1, kernel=sigmoid ................................\n",
      "[CV] .... C=100, gamma=0.1, kernel=sigmoid, score=0.272, total=  34.3s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] ....... C=100, gamma=0.01, kernel=rbf, score=0.317, total=  32.7s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] ....... C=100, gamma=0.01, kernel=rbf, score=0.283, total=  33.4s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] ....... C=100, gamma=0.01, kernel=rbf, score=0.272, total=  37.8s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] ....... C=100, gamma=0.01, kernel=rbf, score=0.326, total=  37.4s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] ....... C=100, gamma=0.01, kernel=rbf, score=0.302, total=  31.7s\n",
      "[CV] C=100, gamma=0.01, kernel=sigmoid ...............................\n",
      "[CV] ... C=100, gamma=0.01, kernel=sigmoid, score=0.354, total=  31.8s\n",
      "[CV] C=100, gamma=0.01, kernel=sigmoid ...............................\n",
      "[CV] ... C=100, gamma=0.01, kernel=sigmoid, score=0.374, total=  31.5s\n",
      "[CV] C=100, gamma=0.01, kernel=sigmoid ...............................\n",
      "[CV] ... C=100, gamma=0.01, kernel=sigmoid, score=0.331, total=  33.0s\n",
      "[CV] C=100, gamma=0.01, kernel=sigmoid ...............................\n",
      "[CV] ... C=100, gamma=0.01, kernel=sigmoid, score=0.410, total=  32.4s\n",
      "[CV] C=100, gamma=0.01, kernel=sigmoid ...............................\n",
      "[CV] ... C=100, gamma=0.01, kernel=sigmoid, score=0.374, total=  30.1s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ...... C=100, gamma=0.001, kernel=rbf, score=0.430, total=  23.5s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ...... C=100, gamma=0.001, kernel=rbf, score=0.458, total=  24.4s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ...... C=100, gamma=0.001, kernel=rbf, score=0.444, total=  24.1s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ...... C=100, gamma=0.001, kernel=rbf, score=0.457, total=  23.8s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ...... C=100, gamma=0.001, kernel=rbf, score=0.412, total=  23.4s\n",
      "[CV] C=100, gamma=0.001, kernel=sigmoid ..............................\n",
      "[CV] .. C=100, gamma=0.001, kernel=sigmoid, score=0.439, total=  27.9s\n",
      "[CV] C=100, gamma=0.001, kernel=sigmoid ..............................\n",
      "[CV] .. C=100, gamma=0.001, kernel=sigmoid, score=0.451, total=  27.8s\n",
      "[CV] C=100, gamma=0.001, kernel=sigmoid ..............................\n",
      "[CV] .. C=100, gamma=0.001, kernel=sigmoid, score=0.441, total=  27.9s\n",
      "[CV] C=100, gamma=0.001, kernel=sigmoid ..............................\n",
      "[CV] .. C=100, gamma=0.001, kernel=sigmoid, score=0.444, total=  27.9s\n",
      "[CV] C=100, gamma=0.001, kernel=sigmoid ..............................\n",
      "[CV] .. C=100, gamma=0.001, kernel=sigmoid, score=0.400, total=  27.5s\n",
      "[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed: 48.4min finished\n",
      "CPU times: user 47min 45s, sys: 42.9 s, total: 48min 28s\n",
      "Wall time: 49min 4s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(class_weight='balanced', random_state=20),\n",
       "             param_grid={'C': [1, 10, 100], 'gamma': [0.1, 0.01, 0.001],\n",
       "                         'kernel': ['rbf', 'sigmoid']},\n",
       "             scoring='f1', verbose=3)"
      ]
     },
     "metadata": {},
     "execution_count": 270
    }
   ],
   "source": [
    "%%time\n",
    "# fit the grid search to our data\n",
    "grid_baseline.fit(tfidf_data_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "F1 Score: 0.44115028504856885\nBest Hyperparameters: {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\nModel object with best parameters: \nSVC(C=1, class_weight='balanced', gamma=0.1, random_state=20)\n"
     ]
    }
   ],
   "source": [
    "# generate score with .best_score_ and hyperparemeters with .best_params_\n",
    "print('F1 Score:', grid_baseline.best_score_)\n",
    "print('Best Hyperparameters:', grid_baseline.best_params_)\n",
    "print('Model object with best parameters: ')\n",
    "print(grid_baseline.best_estimator_)"
   ]
  },
  {
   "source": [
    "The grid search found that the best hyperparameters are `{'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}`.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the response for test dataset\n",
    "grid_base_y_pred_train = grid_baseline.best_estimator_.predict(tfidf_data_train)\n",
    "\n",
    "# predict the training set\n",
    "grid_base_y_pred_test = grid_baseline.best_estimator_.predict(tfidf_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting variables for evaluation metrics\n",
    "grid_precision = precision_score(y_test, grid_base_y_pred_test)\n",
    "grid_recall = recall_score(y_test, grid_base_y_pred_test)\n",
    "grid_f1_score = f1_score(y_test, grid_base_y_pred_test)\n",
    "grid_weighted_f1_score = f1_score(y_test, grid_base_y_pred_test, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing Evaluation Metrics:\nPrecision: 0.2808\nRecall: 0.6129\nF1 Score: 0.3851\nWeighted F1 Score: 0.9083\n"
     ]
    }
   ],
   "source": [
    "evaluation(grid_precision, grid_recall, grid_f1_score, grid_weighted_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding these metrics to evaluation metric dict\n",
    "metric_dict['Grid Search SVM'] = {'precision': grid_precision, 'recall': grid_recall, 'f1_score': grid_f1_score, 'weighted_f1': grid_weighted_f1_score}"
   ]
  },
  {
   "source": [
    "# Evaluating All Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                   precision    recall  f1_score  weighted_f1\n",
       "Baseline Random Forest              0.412844  0.161290  0.231959     0.927249\n",
       "Baseline Logisitic Regression       0.293900  0.569892  0.387805     0.913449\n",
       "Baseline Naive Bayes                0.411765  0.125448  0.192308     0.925487\n",
       "Baseline SVM                        0.360947  0.437276  0.395462     0.928112\n",
       "Baseline SVM with Doc2Vec           0.187896  0.625293  0.288961     0.864053\n",
       "SVM Oversampled with SMOTE          0.339286  0.272401  0.302187     0.925527\n",
       "SVM Undersampled with Tomek Links   0.656250  0.225806  0.336000     0.937993\n",
       "Grid Search SVM                     0.280788  0.612903  0.385135     0.908306"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>f1_score</th>\n      <th>weighted_f1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Baseline Random Forest</th>\n      <td>0.412844</td>\n      <td>0.161290</td>\n      <td>0.231959</td>\n      <td>0.927249</td>\n    </tr>\n    <tr>\n      <th>Baseline Logisitic Regression</th>\n      <td>0.293900</td>\n      <td>0.569892</td>\n      <td>0.387805</td>\n      <td>0.913449</td>\n    </tr>\n    <tr>\n      <th>Baseline Naive Bayes</th>\n      <td>0.411765</td>\n      <td>0.125448</td>\n      <td>0.192308</td>\n      <td>0.925487</td>\n    </tr>\n    <tr>\n      <th>Baseline SVM</th>\n      <td>0.360947</td>\n      <td>0.437276</td>\n      <td>0.395462</td>\n      <td>0.928112</td>\n    </tr>\n    <tr>\n      <th>Baseline SVM with Doc2Vec</th>\n      <td>0.187896</td>\n      <td>0.625293</td>\n      <td>0.288961</td>\n      <td>0.864053</td>\n    </tr>\n    <tr>\n      <th>SVM Oversampled with SMOTE</th>\n      <td>0.339286</td>\n      <td>0.272401</td>\n      <td>0.302187</td>\n      <td>0.925527</td>\n    </tr>\n    <tr>\n      <th>SVM Undersampled with Tomek Links</th>\n      <td>0.656250</td>\n      <td>0.225806</td>\n      <td>0.336000</td>\n      <td>0.937993</td>\n    </tr>\n    <tr>\n      <th>Grid Search SVM</th>\n      <td>0.280788</td>\n      <td>0.612903</td>\n      <td>0.385135</td>\n      <td>0.908306</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 279
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(metric_dict, orient='index')"
   ]
  },
  {
   "source": [
    "Unfortunately, the model with grid searched hyperparameters didn't perform better than the baseline."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Pipeline Final Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Training model on full dataset\n",
    "using .fit_transform(X_lem)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Conclusion"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Next Steps"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}