{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "learn-env",
   "display_name": "learn-env",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Preprocessing Text Data\n",
    "\n",
    "The purpose of this notebook is to conduct the preprocessing steps that are necessary for text data in NLP. This includes tokenizing, removing stop words, vectorizing, etc."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline\n",
    "# packages for NLP preprocessing\n",
    "import nltk\n",
    "from sklearn.feature_extraction import text \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.collocations import *\n",
    "import pickle"
   ]
  },
  {
   "source": [
    "### Loading in the clean dataframe from `data_cleaning.ipynb`\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is our corpus\n",
    "clean_df = pd.read_pickle('../pickle/clean_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   total_votes  hate_speech_votes  other_votes  label  \\\n",
       "0            3                  0            3      0   \n",
       "1            3                  0            3      0   \n",
       "2            3                  0            3      0   \n",
       "3            3                  0            3      0   \n",
       "4            6                  0            6      0   \n",
       "\n",
       "                                               tweet  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...   \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...   \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...   \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...   \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...   \n",
       "\n",
       "                                        clean_tweets  \n",
       "0     as a woman you shouldnt complain about clea...  \n",
       "1     boy dats coldtyga dwn bad for cuffin dat ho...  \n",
       "2     dawg   you ever fuck a bitch and she sta to...  \n",
       "3                             she look like a tranny  \n",
       "4     the shit you hear about me might be true or...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>total_votes</th>\n      <th>hate_speech_votes</th>\n      <th>other_votes</th>\n      <th>label</th>\n      <th>tweet</th>\n      <th>clean_tweets</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n      <td>as a woman you shouldnt complain about clea...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n      <td>boy dats coldtyga dwn bad for cuffin dat ho...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n      <td>dawg   you ever fuck a bitch and she sta to...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n      <td>she look like a tranny</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6</td>\n      <td>0</td>\n      <td>6</td>\n      <td>0</td>\n      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n      <td>the shit you hear about me might be true or...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "source": [
    "### Creating `tweet_df` with only cleaned tweets column"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = clean_df[['clean_tweets', 'label']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                        clean_tweets  label\n",
       "0     as a woman you shouldnt complain about clea...      0\n",
       "1     boy dats coldtyga dwn bad for cuffin dat ho...      0\n",
       "2     dawg   you ever fuck a bitch and she sta to...      0\n",
       "3                             she look like a tranny      0\n",
       "4     the shit you hear about me might be true or...      0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>clean_tweets</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>as a woman you shouldnt complain about clea...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>boy dats coldtyga dwn bad for cuffin dat ho...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>dawg   you ever fuck a bitch and she sta to...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>she look like a tranny</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>the shit you hear about me might be true or...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tweet_df['clean_tweets']\n",
    "target = tweet_df['label']"
   ]
  },
  {
   "source": [
    "## Tokenizing Before Removing Stop Words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "When working with text data, one of the first steps is to remove stop words from the corpus. Although text would be gramatically incorrect without these stop words, they provide little value to models and typically hinder performace.\n",
    "\n",
    "Before removing stop words, let's take a look at the top words in this corpus."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to tokenize without removing stop words\n",
    "def unfiltered_tokens(text):\n",
    "    dirty_tokens = nltk.word_tokenize(text)\n",
    "    return dirty_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying this function to the `clean_tweets` column\n",
    "unfilterd_data = list(map(unfiltered_tokens, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# morphing `unfiltered_data` into a readable list\n",
    "flat_unfiltered = [item for sublist in unfilterd_data for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('a', 9476),\n",
       " ('bitch', 8227),\n",
       " ('i', 7538),\n",
       " ('the', 7168),\n",
       " ('you', 6120),\n",
       " ('to', 5332),\n",
       " ('and', 3951),\n",
       " ('my', 3579),\n",
       " ('that', 3528),\n",
       " ('bitches', 3083),\n",
       " ('in', 3051),\n",
       " ('is', 2909),\n",
       " ('like', 2766),\n",
       " ('me', 2764),\n",
       " ('of', 2544),\n",
       " ('on', 2518),\n",
       " ('be', 2375),\n",
       " ('hoes', 2368),\n",
       " ('this', 2149),\n",
       " ('for', 2119)]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# getting frequency distribution\n",
    "dirty_corpus_freqdist = FreqDist(flat_unfiltered)\n",
    "# top 20 words in the corpus\n",
    "dirty_corpus_freqdist.most_common(20)"
   ]
  },
  {
   "source": [
    "We can see that stop words typically dominate the top spots. Such as 'a', 'i', 'the', 'to', etc. But this tells us nothing about the actual content of the corpus, and will negatively impact model performance.\n",
    "\n",
    "Let's actually remove the stop words now."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Removing Stop Words in Tokenization\n",
    "We can use NLTK's built-in library of stop words to remove them in a tokenizing function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stopwords_removed = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
    "    return stopwords_removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the above function to our data/features \n",
    "processed_data = list(map(process_tweet, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20277"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "total_vocab = set()\n",
    "for comment in processed_data:\n",
    "    total_vocab.update(comment)\n",
    "len(total_vocab)"
   ]
  },
  {
   "source": [
    "Now that the stop words are removed and the corpus is tokenized, let's take a look at the top words in this corpus."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('bitch', 8227),\n",
       " ('bitches', 3083),\n",
       " ('like', 2766),\n",
       " ('hoes', 2368),\n",
       " ('pussy', 2099),\n",
       " ('im', 2061),\n",
       " ('hoe', 1906),\n",
       " ('dont', 1749),\n",
       " ('got', 1597),\n",
       " ('ass', 1570),\n",
       " ('get', 1428),\n",
       " ('fuck', 1411),\n",
       " ('u', 1280),\n",
       " ('shit', 1262),\n",
       " ('nigga', 1198),\n",
       " ('aint', 1158),\n",
       " ('trash', 1142),\n",
       " ('lol', 1074),\n",
       " ('know', 806),\n",
       " ('niggas', 791)]"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# morphing `processed_data` into a readable list\n",
    "flat_filtered = [item for sublist in processed_data for item in sublist]\n",
    "# getting frequency distribution\n",
    "clean_corpus_freqdist = FreqDist(flat_filtered)\n",
    "# top 20 words in cleaned corpus\n",
    "clean_corpus_freqdist.most_common(20)"
   ]
  },
  {
   "source": [
    "We can see that the \"meaningless\" stop words have been removed from the corpus. Some of the words here are still up for debate, but let's run this data through some baseline models and see how they perform."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Lemmatization\n",
    "\n",
    "This last method reduces each word into a linguistically valid **lemma**, or root word. It does this through linguistic mappings, using the WordNet lexical database.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list with all lemmatized outputs\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "lemmatized_output = []\n",
    "\n",
    "for listy in processed_data:\n",
    "    lemmed = ' '.join([lemmatizer.lemmatize(w) for w in listy])\n",
    "    lemmatized_output.append(lemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lem = lemmatized_output\n",
    "y_lem = target"
   ]
  },
  {
   "source": [
    "# pickle these for modeling\n",
    "pickle_out = open('../pickle/X_lem.pkl','wb')\n",
    "pickle.dump(X_lem, pickle_out)\n",
    "pickle_out.close()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lem.to_pickle('../pickle/y_lem.pkl')"
   ]
  },
  {
   "source": [
    "### Now `X_lem` and `y_lem` are ready to be brought over to the modeling notebooks."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Additional Corpus EDA - Creating Bigrams\n",
    "\n",
    "Knowing individual word frequencies is somewhat informative, but in practice, some of these tokens are actually parts of larger phrases that should be treated as a single unit. Let's create some bigrams, and see which combinations of words are most telling."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a finder & passing in tokenized corpus\n",
    "bigram_finder = BigramCollocationFinder.from_words(flat_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing bigram scores\n",
    "corpus_scored = bigram_finder.score_ngrams(bigram_measures.raw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(('wan', 'na'), 0.0018295365531745946),\n",
       " (('got', 'ta'), 0.001657849838507184),\n",
       " (('gon', 'na'), 0.0015666412713401222),\n",
       " (('bitch', 'ass'), 0.0012769199403388667),\n",
       " (('like', 'bitch'), 0.0011910765830051614),\n",
       " (('bitch', 'im'), 0.001169615743671735),\n",
       " (('look', 'like'), 0.0010837723863380297),\n",
       " (('ass', 'bitch'), 0.0010730419666713166),\n",
       " (('yo', 'bitch'), 0.0010301202880044638),\n",
       " (('bad', 'bitch'), 0.0009711029798375415),\n",
       " (('bitch', 'dont'), 0.0009603725601708283),\n",
       " (('bad', 'bitches'), 0.0007672250061699913),\n",
       " (('bitch', 'bitch'), 0.0007618597963366347),\n",
       " (('ass', 'nigga'), 0.0007135729078364255),\n",
       " (('hoes', 'aint'), 0.0006921120685029991),\n",
       " (('bitch', 'got'), 0.000681381648836286),\n",
       " (('fuck', 'bitch'), 0.000681381648836286),\n",
       " (('bitch', 'aint'), 0.0006706512291695728),\n",
       " (('little', 'bitch'), 0.0006652860193362163),\n",
       " (('bitch', 'nigga'), 0.0005687122423357978)]"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "# top 20 bigrams\n",
    "corpus_scored[:20]"
   ]
  },
  {
   "source": [
    "Is seems that most of these are nonsensical phrases, and may indicate that the data is still dirty. However, let's start running some baseline models to see how they perform with this data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}